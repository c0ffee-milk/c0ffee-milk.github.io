<!DOCTYPE html>
<html lang="en">
    <head prefix="og: https://ogp.me/ns#">
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="color-scheme" content="light dark">
  
  <title>pytorch 简易入门 - Hexo</title>
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    <link rel='manifest' href='/manifest.json'>
  

  
  
  
  <meta property="og:title" content="pytorch 简易入门 - Hexo" />
  
  <meta property="og:type" content="article" />
  
  <meta property="og:url" content="http://example.com/2025/06/29/pytorch/index.html" />
  
  <meta property="og:image" content="/favicon.png" />
  
  <meta property="og:article:published_time" content="2025-06-29T13:08:08.000Z" />
  
  <meta property="og:article:author" content="Latteo" />
  
  

  
<link rel="stylesheet" href="/css/var.css">

  
<link rel="stylesheet" href="/css/main.css">

  
<link rel="stylesheet" href="/css/typography.css">

  
<link rel="stylesheet" href="/css/code-highlighting.css">

  
<link rel="stylesheet" href="/css/components.css">

  
<link rel="stylesheet" href="/css/nav.css">

  
<link rel="stylesheet" href="/css/paginator.css">

  
<link rel="stylesheet" href="/css/footer.css">

  
<link rel="stylesheet" href="/css/post-list.css">

  
  
  
<link rel="stylesheet" href="/css/toc.css">

  
  
  
  
  
<link rel="stylesheet" href="/css/post.css">

  
  
  
  
  
  
<link rel="stylesheet" href="/css/main.css">

  
<link rel="stylesheet" href="/css/nav.css">

  
<link rel="stylesheet" href="/css/components.css">

  
<link rel="stylesheet" href="/css/categories.css">

  
<link rel="stylesheet" href="/css/tags.css">

  
<link rel="stylesheet" href="/css/archive.css">

  
<link rel="stylesheet" href="/css/post.css">

  
<link rel="stylesheet" href="/css/code-highlighting.css">

  
<link rel="stylesheet" href="/css/giscus.css">


    

    

  
<meta name="generator" content="Hexo 7.3.0"></head>
    <body
        data-color-scheme="auto"
        data-uppercase-categories="true"
        
        data-config-root="/"
        
        data-toc="true"
        data-toc-max-depth="2"
        
        
    >
        <nav id="theme-nav">
    <div class="inner">
        <a class="title" href="/">Blog</a>
        <div class="nav-arrow"></div>
        <div class="nav-items">
            <a class="nav-item nav-item-home" href="/">Home</a>
            
            
            <a class="nav-item" href="/archives">Archives</a>
            
            
            
            <a class="nav-item" href="/projects">Projects</a>
            
            
            
            <a class="nav-item" href="/about">About</a>
            
            
            
            <a class="nav-item nav-item-github nav-item-icon" href="https://github.com/c0ffee-milk" target="_blank" aria-label="GitHub">&nbsp;</a>
            
            
            
            <a class="nav-item nav-item-search nav-item-icon" href="/search" target="_blank" aria-label="Search">&nbsp;</a>
            
            
        </div>
    </div>
</nav>
        
<article class="post">
    <div class="meta">
        

        
        <div class="date" id="date">
            <span>June</span>
            <span>29,</span>
            <span>2025</span>
        </div>
        

        <h1 class="title">pytorch 简易入门</h1>
    </div>

    <div class="divider"></div>

    <div class="content">
        <h2 id="1-PyTorch-的基本概念"><a href="#1-PyTorch-的基本概念" class="headerlink" title="1. PyTorch 的基本概念"></a><strong>1. PyTorch 的基本概念</strong></h2><h3 id="1-1-Tensor"><a href="#1-1-Tensor" class="headerlink" title="1.1 Tensor"></a>1.1 Tensor</h3><p>Tensor类似与NumPy的ndarray，但是可以用GPU加速。</p>
<ol>
<li><p>构造未初始化矩阵 <code>empty</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">x = torch.empty(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"></span><br><span class="line">tensor([[<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>]])</span><br></pre></td></tr></table></figure>
</li>
<li><p>随机初始化一个矩阵 <code>rand</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"></span><br><span class="line">tensor([[<span class="number">0.6819</span>, <span class="number">0.1574</span>, <span class="number">0.6951</span>],</span><br><span class="line">        [<span class="number">0.9938</span>, <span class="number">0.7842</span>, <span class="number">0.3295</span>],</span><br><span class="line">        [<span class="number">0.6799</span>, <span class="number">0.4662</span>, <span class="number">0.9871</span>],</span><br><span class="line">        [<span class="number">0.1258</span>, <span class="number">0.4621</span>, <span class="number">0.7077</span>],</span><br><span class="line">        [<span class="number">0.3343</span>, <span class="number">0.1081</span>, <span class="number">0.6953</span>]])</span><br></pre></td></tr></table></figure>
</li>
<li><p>构造一个零初始化矩阵 <code>zeros</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">x = torch.zeros(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"></span><br><span class="line">tensor([[<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>]])</span><br></pre></td></tr></table></figure>
</li>
<li><p>构造一个指定数据类型的矩阵 <code>dtype</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand(<span class="number">5</span>, <span class="number">3</span>, dtype=torch.float32)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"></span><br><span class="line">tensor([[<span class="number">0.9735</span>, <span class="number">0.6720</span>, <span class="number">0.1250</span>],</span><br><span class="line">        [<span class="number">0.3461</span>, <span class="number">0.6952</span>, <span class="number">0.1919</span>],</span><br><span class="line">        [<span class="number">0.7887</span>, <span class="number">0.4517</span>, <span class="number">0.9117</span>],</span><br><span class="line">        [<span class="number">0.5153</span>, <span class="number">0.8150</span>, <span class="number">0.3046</span>],</span><br><span class="line">        [<span class="number">0.5150</span>, <span class="number">0.4235</span>, <span class="number">0.8831</span>]])</span><br></pre></td></tr></table></figure>
</li>
<li><p>使用数组构造Tensor</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">5.5</span>, <span class="number">3</span>])</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"></span><br><span class="line">tensor([<span class="number">5.5000</span>, <span class="number">3.0000</span>])</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看一个Tensor的shape <code>size</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">x = torch.zeros(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="built_in">print</span>(x.size())</span><br><span class="line"></span><br><span class="line">tensor([[<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>]])</span><br><span class="line">torch.Size([<span class="number">5</span>, <span class="number">3</span>])</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="1-2-Operation"><a href="#1-2-Operation" class="headerlink" title="1.2 Operation"></a>1.2 Operation</h3><p>Operation一般可以使用函数的方式使用，以加法为例</p>
<ol>
<li><p>用运算符实现加法 <code>+</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">y = torch.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="built_in">print</span>(y)</span><br><span class="line"><span class="built_in">print</span>(x + y)</span><br><span class="line"></span><br><span class="line">tensor([[<span class="number">0.8217</span>, <span class="number">0.7760</span>, <span class="number">0.0340</span>],</span><br><span class="line">        [<span class="number">0.0413</span>, <span class="number">0.5867</span>, <span class="number">0.6914</span>],</span><br><span class="line">        [<span class="number">0.4074</span>, <span class="number">0.2192</span>, <span class="number">0.8599</span>],</span><br><span class="line">        [<span class="number">0.1837</span>, <span class="number">0.4944</span>, <span class="number">0.1833</span>],</span><br><span class="line">        [<span class="number">0.9589</span>, <span class="number">0.1776</span>, <span class="number">0.0896</span>]])</span><br><span class="line">tensor([[<span class="number">0.5083</span>, <span class="number">0.3603</span>, <span class="number">0.0964</span>],</span><br><span class="line">        [<span class="number">0.5016</span>, <span class="number">0.6730</span>, <span class="number">0.5046</span>],</span><br><span class="line">        [<span class="number">0.7970</span>, <span class="number">0.7357</span>, <span class="number">0.1990</span>],</span><br><span class="line">        [<span class="number">0.2162</span>, <span class="number">0.4779</span>, <span class="number">0.5341</span>],</span><br><span class="line">        [<span class="number">0.8595</span>, <span class="number">0.5517</span>, <span class="number">0.4641</span>]])</span><br><span class="line">tensor([[<span class="number">1.3300</span>, <span class="number">1.1363</span>, <span class="number">0.1304</span>],</span><br><span class="line">        [<span class="number">0.5429</span>, <span class="number">1.2597</span>, <span class="number">1.1960</span>],</span><br><span class="line">        [<span class="number">1.2044</span>, <span class="number">0.9549</span>, <span class="number">1.0589</span>],</span><br><span class="line">        [<span class="number">0.3999</span>, <span class="number">0.9722</span>, <span class="number">0.7175</span>],</span><br><span class="line">        [<span class="number">1.8184</span>, <span class="number">0.7293</span>, <span class="number">0.5537</span>]])</span><br></pre></td></tr></table></figure>
</li>
<li><p>用函数实现加法 <code>torch.add()</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">y = torch.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="built_in">print</span>(y)</span><br><span class="line"><span class="built_in">print</span>(torch.add(x, y))</span><br><span class="line"></span><br><span class="line">tensor([[<span class="number">0.4643</span>, <span class="number">0.0701</span>, <span class="number">0.3483</span>],</span><br><span class="line">        [<span class="number">0.1846</span>, <span class="number">0.7678</span>, <span class="number">0.8397</span>],</span><br><span class="line">        [<span class="number">0.9759</span>, <span class="number">0.9102</span>, <span class="number">0.5844</span>],</span><br><span class="line">        [<span class="number">0.2472</span>, <span class="number">0.2512</span>, <span class="number">0.6340</span>],</span><br><span class="line">        [<span class="number">0.2352</span>, <span class="number">0.7352</span>, <span class="number">0.9725</span>]])</span><br><span class="line">tensor([[<span class="number">0.0229</span>, <span class="number">0.0309</span>, <span class="number">0.7605</span>],</span><br><span class="line">        [<span class="number">0.5113</span>, <span class="number">0.8178</span>, <span class="number">0.3730</span>],</span><br><span class="line">        [<span class="number">0.8731</span>, <span class="number">0.6131</span>, <span class="number">0.5512</span>],</span><br><span class="line">        [<span class="number">0.7176</span>, <span class="number">0.8111</span>, <span class="number">0.4682</span>],</span><br><span class="line">        [<span class="number">0.5177</span>, <span class="number">0.2994</span>, <span class="number">0.3149</span>]])</span><br><span class="line">tensor([[<span class="number">0.4872</span>, <span class="number">0.1010</span>, <span class="number">1.1088</span>],</span><br><span class="line">        [<span class="number">0.6959</span>, <span class="number">1.5856</span>, <span class="number">1.2127</span>],</span><br><span class="line">        [<span class="number">1.8490</span>, <span class="number">1.5233</span>, <span class="number">1.1356</span>],</span><br><span class="line">        [<span class="number">0.9649</span>, <span class="number">1.0623</span>, <span class="number">1.1022</span>],</span><br><span class="line">        [<span class="number">0.7528</span>, <span class="number">1.0346</span>, <span class="number">1.2874</span>]])</span><br></pre></td></tr></table></figure>
</li>
<li><p>为加法提供返回值 <code>out=</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">y = torch.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">result = torch.empty(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">torch.add(x, y, out=result)</span><br><span class="line"><span class="built_in">print</span>(result)</span><br><span class="line"></span><br><span class="line">tensor([[<span class="number">1.3815</span>, <span class="number">1.2649</span>, <span class="number">1.3772</span>],</span><br><span class="line">        [<span class="number">0.8191</span>, <span class="number">1.0258</span>, <span class="number">0.7270</span>],</span><br><span class="line">        [<span class="number">1.5077</span>, <span class="number">1.2752</span>, <span class="number">0.8296</span>],</span><br><span class="line">        [<span class="number">0.8453</span>, <span class="number">1.0702</span>, <span class="number">1.3118</span>],</span><br><span class="line">        [<span class="number">1.0655</span>, <span class="number">1.7370</span>, <span class="number">0.9018</span>]])</span><br></pre></td></tr></table></figure>
</li>
<li><p>原地修改Tensor <code>add_(以下划线结尾)</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">y = torch.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="built_in">print</span>(y)</span><br><span class="line"><span class="built_in">print</span>(y.add_(x))</span><br><span class="line"></span><br><span class="line">tensor([[<span class="number">0.1245</span>, <span class="number">0.0032</span>, <span class="number">0.2941</span>],</span><br><span class="line">        [<span class="number">0.6233</span>, <span class="number">0.5174</span>, <span class="number">0.7957</span>],</span><br><span class="line">        [<span class="number">0.6105</span>, <span class="number">0.4572</span>, <span class="number">0.8840</span>],</span><br><span class="line">        [<span class="number">0.6559</span>, <span class="number">0.8040</span>, <span class="number">0.7680</span>],</span><br><span class="line">        [<span class="number">0.0818</span>, <span class="number">0.1590</span>, <span class="number">0.2294</span>]])</span><br><span class="line">tensor([[<span class="number">0.6524</span>, <span class="number">0.8919</span>, <span class="number">0.6358</span>],</span><br><span class="line">        [<span class="number">0.8619</span>, <span class="number">0.3013</span>, <span class="number">0.1395</span>],</span><br><span class="line">        [<span class="number">0.7151</span>, <span class="number">0.4323</span>, <span class="number">0.2184</span>],</span><br><span class="line">        [<span class="number">0.5981</span>, <span class="number">0.9320</span>, <span class="number">0.9239</span>],</span><br><span class="line">        [<span class="number">0.0647</span>, <span class="number">0.4407</span>, <span class="number">0.3301</span>]])</span><br><span class="line">tensor([[<span class="number">0.7769</span>, <span class="number">0.8951</span>, <span class="number">0.9299</span>],</span><br><span class="line">        [<span class="number">1.4853</span>, <span class="number">0.8186</span>, <span class="number">0.9352</span>],</span><br><span class="line">        [<span class="number">1.3256</span>, <span class="number">0.8895</span>, <span class="number">1.1024</span>],</span><br><span class="line">        [<span class="number">1.2540</span>, <span class="number">1.7360</span>, <span class="number">1.6919</span>],</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="1-3-Tensor的变换"><a href="#1-3-Tensor的变换" class="headerlink" title="1.3 Tensor的变换"></a>1.3 Tensor的变换</h3><ol>
<li><p>resize和reshape一个Tensor <code>torch.view</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line">y = x.view(-<span class="number">1</span>, <span class="number">2</span>) <span class="comment"># -1表示让Pytorch自己推断该维的大小</span></span><br><span class="line">z = x.view(<span class="number">16</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="built_in">print</span>(y)</span><br><span class="line"><span class="built_in">print</span>(z)</span><br><span class="line"></span><br><span class="line">tensor([[<span class="number">0.7446</span>, <span class="number">0.6093</span>, <span class="number">0.6807</span>, <span class="number">0.3331</span>],</span><br><span class="line">        [<span class="number">0.5358</span>, <span class="number">0.1826</span>, <span class="number">0.0263</span>, <span class="number">0.8574</span>],</span><br><span class="line">        [<span class="number">0.6927</span>, <span class="number">0.6911</span>, <span class="number">0.7574</span>, <span class="number">0.4551</span>],</span><br><span class="line">        [<span class="number">0.0486</span>, <span class="number">0.9530</span>, <span class="number">0.8304</span>, <span class="number">0.0535</span>]])</span><br><span class="line">tensor([[<span class="number">0.7446</span>, <span class="number">0.6093</span>],</span><br><span class="line">        [<span class="number">0.6807</span>, <span class="number">0.3331</span>],</span><br><span class="line">        [<span class="number">0.5358</span>, <span class="number">0.1826</span>],</span><br><span class="line">        [<span class="number">0.0263</span>, <span class="number">0.8574</span>],</span><br><span class="line">        [<span class="number">0.6927</span>, <span class="number">0.6911</span>],</span><br><span class="line">        [<span class="number">0.7574</span>, <span class="number">0.4551</span>],</span><br><span class="line">        [<span class="number">0.0486</span>, <span class="number">0.9530</span>],</span><br><span class="line">        [<span class="number">0.8304</span>, <span class="number">0.0535</span>]])</span><br><span class="line">tensor([<span class="number">0.7446</span>, <span class="number">0.6093</span>, <span class="number">0.6807</span>, <span class="number">0.3331</span>, <span class="number">0.5358</span>, <span class="number">0.1826</span>, <span class="number">0.0263</span>, <span class="number">0.8574</span>, <span class="number">0.6927</span>,</span><br><span class="line">        <span class="number">0.6911</span>, <span class="number">0.7574</span>, <span class="number">0.4551</span>, <span class="number">0.0486</span>, <span class="number">0.9530</span>, <span class="number">0.8304</span>, <span class="number">0.0535</span>])</span><br></pre></td></tr></table></figure>
</li>
<li><p>将单个元素的Tensor转化为Python number <code>torch.item</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="comment">#输出的是一个Tensor</span></span><br><span class="line">tensor([-<span class="number">0.6966</span>])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x.item())</span><br><span class="line"><span class="comment">#输出的是一个数</span></span><br><span class="line">-<span class="number">0.6966081857681274</span></span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="1-4-Tensor-与-Numpy-的相互转换"><a href="#1-4-Tensor-与-Numpy-的相互转换" class="headerlink" title="1.4 Tensor 与 Numpy 的相互转换"></a>1.4 Tensor 与 Numpy 的相互转换</h3><p>Tensor 与 Numpy 共享内存地址，当其中一方被修改时也会影响另一方。</p>
<p>把NumPy数组转成Torch Tensor <code>torch.from_numpy()</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a = np.ones(<span class="number">5</span>)</span><br><span class="line">b = torch.from_numpy(a)</span><br><span class="line">np.add(a, <span class="number">1</span>, out=a)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="comment"># [2. 2. 2. 2. 2.]</span></span><br><span class="line"><span class="built_in">print</span>(b)</span><br><span class="line"><span class="comment"># tensor([ 2.,  2.,  2.,  2.,  2.], dtype=torch.float64)</span></span><br></pre></td></tr></table></figure>

<h3 id="1-5-CUDA-Tensor"><a href="#1-5-CUDA-Tensor" class="headerlink" title="1.5 CUDA Tensor"></a>1.5 CUDA Tensor</h3><p>Tensor可以使用to()方法来移到任意设备上</p>
<p>如果设备有cuda，使用 <code>torch.device</code>  将tensors放在GPU上</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">        device = torch.device(<span class="string">&quot;cuda&quot;</span>)</span><br><span class="line">        y = torch.ones_like(x, device=device)</span><br><span class="line">        x = x.to(device)</span><br><span class="line">        z = x + y</span><br><span class="line">        <span class="built_in">print</span>(z)</span><br><span class="line">        <span class="built_in">print</span>(z.to(<span class="string">&quot;cpu&quot;</span>, torch.double))</span><br></pre></td></tr></table></figure>

<h3 id="1-6-Autograd-自动求导"><a href="#1-6-Autograd-自动求导" class="headerlink" title="1.6 Autograd 自动求导"></a>1.6 Autograd 自动求导</h3><p>Autograd 包是 Pytorch 的核心，Autograd为所有用于Tensor的operation提供自动求导的功能。</p>
<ol>
<li>反向传播 torch.Tensor 是 Autograd包的核心类，若其属性requires_grad 为True，PyTorch就会追踪所有与之相关的operation。当完成(正向)计算之后， 我们可以调用backward()，PyTorch会自动的把所有的梯度都计算好。与这个tensor相关的梯度都会累加到它的grad属性里。</li>
<li>禁用反向传播 如果不想计算这个tensor的梯度，我们可以调用detach()，这样它就不会参与梯度的计算了。为了阻止PyTorch记录用于梯度计算相关的信息(从而节约内存)，我们可以使用 with torch.no_grad()。这在模型的推理时非常有用，因为推理的时候我们不需要计算梯度，否则我们就得一个个的修改Tensor的requires_grad属性，这会非常麻烦。</li>
<li>计算梯度 如果希望想计算梯度，可以对一个Tensor调用它的backward()方法。如果这个Tensor是一个scalar(只有一个数)，那么调用时不需要传任何参数。如果Tensor多于一个数，那么需要传入和它的shape一样的参数，表示反向传播过来的梯度。</li>
</ol>
<p>示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建tensor时设置属性requires_grad=True</span></span><br><span class="line">x = torch.ones(<span class="number">2</span>, <span class="number">2</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过Operation产生新的tensor</span></span><br><span class="line">y = x + <span class="number">2</span></span><br><span class="line"><span class="built_in">print</span>(y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过Operation产生的tensor，其grad_fn不为None</span></span><br><span class="line"><span class="built_in">print</span>(y.grad_fn)</span><br><span class="line"></span><br><span class="line">z = y * y * <span class="number">3</span></span><br><span class="line">out = z.mean()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(z)</span><br><span class="line"><span class="built_in">print</span>(out)</span><br><span class="line"></span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>]], requires_grad=<span class="literal">True</span>)</span><br><span class="line">tensor([[<span class="number">3.</span>, <span class="number">3.</span>],</span><br><span class="line">        [<span class="number">3.</span>, <span class="number">3.</span>]], grad_fn=&lt;AddBackward0&gt;)</span><br><span class="line">&lt;AddBackward0 <span class="built_in">object</span> at <span class="number">0x1007be670</span>&gt;</span><br><span class="line">tensor([[<span class="number">27.</span>, <span class="number">27.</span>],</span><br><span class="line">        [<span class="number">27.</span>, <span class="number">27.</span>]], grad_fn=&lt;MulBackward0&gt;)</span><br><span class="line">tensor(<span class="number">27.</span>, grad_fn=&lt;MeanBackward0&gt;)</span><br></pre></td></tr></table></figure>

<p>使用 <code>requires_grad_()</code>  改变一个tensor的requires_grad</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">a = ((a * <span class="number">3</span>) / (a - <span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(a.requires_grad)</span><br><span class="line">a.requires_grad_(<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(a.requires_grad)</span><br><span class="line">b = (a * a).<span class="built_in">sum</span>()</span><br><span class="line"><span class="built_in">print</span>(b.grad_fn)</span><br><span class="line"></span><br><span class="line"><span class="literal">False</span></span><br><span class="line"><span class="literal">True</span></span><br><span class="line">&lt;SumBackward0 <span class="built_in">object</span> at <span class="number">0x102b36670</span>&gt;</span><br></pre></td></tr></table></figure>

<h3 id="1-7-梯度"><a href="#1-7-梯度" class="headerlink" title="1.7 梯度"></a>1.7 梯度</h3><p>使用 <code>backword()</code> 反向计算梯度</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">out.backward()</span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br><span class="line"></span><br><span class="line">tensor([[<span class="number">4.5000</span>, <span class="number">4.5000</span>],</span><br><span class="line">        [<span class="number">4.5000</span>, <span class="number">4.5000</span>]])</span><br></pre></td></tr></table></figure>

<p>使用 <code>with torch.no_grad()</code> 停止计算梯度</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">x = torch.ones(<span class="number">2</span>, <span class="number">2</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(x.requires_grad)</span><br><span class="line"><span class="built_in">print</span>((x ** <span class="number">2</span>).requires_grad)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">  <span class="built_in">print</span>((x ** <span class="number">2</span>).requires_grad)</span><br><span class="line"></span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="literal">False</span></span><br></pre></td></tr></table></figure>

<h2 id="2-PyTorch-神经网络"><a href="#2-PyTorch-神经网络" class="headerlink" title="2. PyTorch 神经网络"></a><strong>2. PyTorch 神经网络</strong></h2><p>神经网络可以通过torch.nn包来创建。我们之前简单的了解了autograd，而nn会使用autograd来定义模型以及求梯度。一个nn.Module对象包括了许多网络层(layer)，并且有一个forward(input)方法来返回output。</p>
<p>下图是一个识别mnist图片的卷积网络结构：</p>
<p><img src="https://raw.githubusercontent.com/c0ffee-milk/photo/master/202506292242286.png" alt="mnist"></p>
<p>训练一个神经网络通常需要如下步骤：</p>
<ol>
<li><p>定义一个神经网络，它通常有一些可以训练的参数</p>
</li>
<li><p>迭代一个数据集(dataset)</p>
</li>
<li><p>处理网络的输入</p>
</li>
<li><p>计算loss(会调用Module对象的forward方法)</p>
</li>
<li><p>计算loss对参数的梯度</p>
</li>
<li><p>更新参数，通常使用如下的梯度下降方法来更新：</p>
<p>weight &#x3D; weight - learning_rate * gradient</p>
</li>
</ol>
<h3 id="2-1-定义网络"><a href="#2-1-定义网络" class="headerlink" title="2.1 定义网络"></a>2.1 定义网络</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(nn.Module):</span><br><span class="line">      <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">              <span class="built_in">super</span>(Net, <span class="variable language_">self</span>).__init__()</span><br><span class="line">              <span class="comment"># 第一层卷积层，输入1个通道（灰度图），输出6个通道（feature map），使用5*5的卷积核</span></span><br><span class="line">              <span class="variable language_">self</span>.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line">              <span class="comment"># 第二层卷积层，输入6个通道，输出16个通道，使用5*5的卷积核</span></span><br><span class="line">              <span class="variable language_">self</span>.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">              <span class="comment"># 全连接层</span></span><br><span class="line">              <span class="variable language_">self</span>.fc1 = nn.Linear(<span class="number">16</span>*<span class="number">5</span>*<span class="number">5</span>, <span class="number">120</span>)</span><br><span class="line">              <span class="variable language_">self</span>.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">              <span class="variable language_">self</span>.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">      <span class="comment"># 前向传播</span></span><br><span class="line">      <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">              <span class="comment"># 第一层卷积+池化层 32x32 -&gt; 28x28 -&gt; 14x14</span></span><br><span class="line">              x = F.max_pool2d(F.relu(<span class="variable language_">self</span>.conv1(x)), (<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">              <span class="comment"># 第二层卷积+池化层 14x14 -&gt; 10x10 -&gt; 5x5</span></span><br><span class="line">              x = F.max_pool2d(F.relu(<span class="variable language_">self</span>.conv2(x)), <span class="number">2</span>) <span class="comment"># 2与（2， 2）效果相同</span></span><br><span class="line">              x = x.view(-<span class="number">1</span>, <span class="variable language_">self</span>.num_flat_features(x)) <span class="comment"># 将多维张量平铺为二维张量</span></span><br><span class="line">              x = F.relu(<span class="variable language_">self</span>.fc1(x))</span><br><span class="line">              x = F.relu(<span class="variable language_">self</span>.fc2(x))</span><br><span class="line">              x = <span class="variable language_">self</span>.fc3(x)</span><br><span class="line">              <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">      <span class="keyword">def</span> <span class="title function_">num_flat_features</span>(<span class="params">self, x</span>):</span><br><span class="line">              size = x.size()[<span class="number">1</span>:]  <span class="comment"># 除了batch维度之外的其它维度。</span></span><br><span class="line">              num_features = <span class="number">1</span></span><br><span class="line">              <span class="keyword">for</span> s <span class="keyword">in</span> size:</span><br><span class="line">              num_features *= s</span><br><span class="line">              <span class="keyword">return</span> num_features</span><br><span class="line"></span><br><span class="line">net = Net()</span><br><span class="line"><span class="built_in">print</span>(net)</span><br><span class="line"></span><br><span class="line">Net(</span><br><span class="line">  (conv1): Conv2d(<span class="number">1</span>, <span class="number">6</span>, kernel_size=(<span class="number">5</span>, <span class="number">5</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (conv2): Conv2d(<span class="number">6</span>, <span class="number">16</span>, kernel_size=(<span class="number">5</span>, <span class="number">5</span>), stride=(<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">  (fc1): Linear(in_features=<span class="number">400</span>, out_features=<span class="number">120</span>, bias=<span class="literal">True</span>)</span><br><span class="line">  (fc2): Linear(in_features=<span class="number">120</span>, out_features=<span class="number">84</span>, bias=<span class="literal">True</span>)</span><br><span class="line">  (fc3): Linear(in_features=<span class="number">84</span>, out_features=<span class="number">10</span>, bias=<span class="literal">True</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>我们只需要定义forward函数，而backward函数会自动通过autograd创建。在forward函数里可以使用任何处理Tensor的函数。我们可以使用函数net.parameters()来得到模型所有的参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">params = <span class="built_in">list</span>(net.parameters())</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(params))</span><br><span class="line"><span class="comment"># 10</span></span><br><span class="line"><span class="built_in">print</span>(params[<span class="number">0</span>].size())  <span class="comment"># conv1的weight</span></span><br><span class="line"><span class="comment"># torch.Size([6, 1, 5, 5])</span></span><br></pre></td></tr></table></figure>

<h3 id="2-2-测试网络"><a href="#2-2-测试网络" class="headerlink" title="2.2 测试网络"></a>2.2 测试网络</h3><p>这个网络(LeNet)期望的输入大小是32x32。如果使用MNIST数据集(28x28)，我们需要缩放到32x32。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">1</span>, <span class="number">1</span>, <span class="number">32</span>, <span class="number">32</span>)</span><br><span class="line">out = net(<span class="built_in">input</span>)</span><br><span class="line"><span class="built_in">print</span>(out)</span><br><span class="line"></span><br><span class="line">tensor([[-<span class="number">0.0279</span>, -<span class="number">0.0748</span>,  <span class="number">0.0013</span>,  <span class="number">0.0537</span>,  <span class="number">0.0150</span>, -<span class="number">0.1020</span>, <span class="number">0.0109</span>, -<span class="number">0.0757</span>, -<span class="number">0.0951</span>,  <span class="number">0.0703</span>]], grad_fn=&lt;AddmmBackward0&gt;)</span><br></pre></td></tr></table></figure>

<p>默认的梯度会累加，因此我们通常在backward之前清除掉之前的梯度值：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net.zero_grad()</span><br><span class="line">out.backward(torch.randn(<span class="number">1</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure>

<h3 id="2-3-损失函数"><a href="#2-3-损失函数" class="headerlink" title="2.3 损失函数"></a>2.3 损失函数</h3><p>损失函数的参数是(output, target)对，output是模型的预测，target是实际的值。损失函数会计算预测值和真实值的差别，损失越小说明预测的越准。</p>
<p>最简单的一个损失函数是：nn.MSELoss，它会计算预测值和真实值的均方误差。比如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">output = net(<span class="built_in">input</span>)</span><br><span class="line">target = torch.arange(<span class="number">1</span>, <span class="number">11</span>)</span><br><span class="line">target = target.view(<span class="number">1</span>, -<span class="number">1</span>)</span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line"></span><br><span class="line">loss = criterion(output, target)</span><br><span class="line"><span class="built_in">print</span>(loss)</span><br></pre></td></tr></table></figure>

<h3 id="2-4-计算梯度"><a href="#2-4-计算梯度" class="headerlink" title="2.4 计算梯度"></a>2.4 计算梯度</h3><p>在调用loss.backward()之前，我们需要清除掉tensor里之前的梯度，否则会累加进去。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">net.zero_grad()     <span class="comment"># 清掉tensor里缓存的梯度值。</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;conv1.bias.grad before backward&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(net.conv1.bias.grad)</span><br><span class="line"></span><br><span class="line">loss.backward()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;conv1.bias.grad after backward&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(net.conv1.bias.grad)</span><br></pre></td></tr></table></figure>

<h3 id="2-5-更新参数"><a href="#2-5-更新参数" class="headerlink" title="2.5 更新参数"></a>2.5 更新参数</h3><p>更新参数最简单的方法是使用随机梯度下降(SGD)： 𝑤𝑒𝑖𝑔ℎ𝑡&#x3D;𝑤𝑒𝑖𝑔ℎ𝑡−𝑙𝑒𝑎𝑟𝑛𝑖𝑛𝑔𝑟𝑎𝑡𝑒∗𝑔𝑟𝑎𝑑𝑖𝑒𝑛𝑡</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line"><span class="keyword">for</span> f <span class="keyword">in</span> net.parameters():</span><br><span class="line">  f.data.sub_(f.grad.data * learning_rate)</span><br></pre></td></tr></table></figure>

<p>通常我们会使用更加复杂的优化方法，比如SGD, Nesterov-SGD, Adam, RMSProp等等。为了实现这些算法，我们可以使用torch.optim包，它的用法也非常简单：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建optimizer需要传入参数和learning rate</span></span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 清除梯度</span></span><br><span class="line">optimizer.zero_grad()</span><br><span class="line">output = net(<span class="built_in">input</span>)</span><br><span class="line">loss = criterion(output, target)</span><br><span class="line">loss.backward()</span><br><span class="line">optimizer.step()    <span class="comment"># optimizer会自动帮我们更新参数</span></span><br></pre></td></tr></table></figure>

<h2 id="3-训练一个分类器"><a href="#3-训练一个分类器" class="headerlink" title="3. 训练一个分类器"></a><strong>3. 训练一个分类器</strong></h2><h3 id="3-1-数据处理"><a href="#3-1-数据处理" class="headerlink" title="3.1 数据处理"></a>3.1 数据处理</h3><p>一般地，当我们处理图片、文本、音频或者视频数据的时候，我们可以使用python代码来把它转换成numpy数组。然后再把numpy数组转换成torch.xxxTensor。</p>
<p>CIFAR10数据集包括十个类别：”airplane”, “automobile”, “bird”, “cat”, “deer”, “dog”, “frog”, “horse”, “ship”,”truck”。图像的对象是3x32x32，也就是3通道(RGB)的32x32的图片。</p>
<p><img src="https://raw.githubusercontent.com/c0ffee-milk/photo/master/202507041145212.png" alt="cifar10"></p>
<h3 id="3-2-训练步骤"><a href="#3-2-训练步骤" class="headerlink" title="3.2 训练步骤"></a>3.2 训练步骤</h3><ol>
<li>使用torchvision加载和预处理CIFAR10训练和测试数据集。</li>
<li>定义卷积网络</li>
<li>定义损失函数</li>
<li>用训练数据训练模型</li>
<li>用测试数据测试模型</li>
</ol>
<h3 id="3-3-加载数据"><a href="#3-3-加载数据" class="headerlink" title="3.3 加载数据"></a>3.3 加载数据</h3><p>使用torchvision加载CIFAR10数据集：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br></pre></td></tr></table></figure>

<p>torchvision读取的datasets是PILImage对象，它的取值范围是[0, 1]，我们把它转换到范围[-1, 1]。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入必要的图像变换模块（transforms）和数据集模块（如 CIFAR10）</span></span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义图像预处理操作：将图像转换为张量，并进行标准化</span></span><br><span class="line">transform = transforms.Compose(</span><br><span class="line">    [</span><br><span class="line">        <span class="comment"># 将 PIL 图像或 numpy 数组转换为 Tensor 张量</span></span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 对图像进行标准化处理：</span></span><br><span class="line">        <span class="comment"># 每个通道减去均值，再除以标准差</span></span><br><span class="line">        <span class="comment"># 这里使用的是 CIFAR-10 数据集的经验参数</span></span><br><span class="line">        transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>))  <span class="comment"># 输入范围 [0,1] → 输出范围 [-1,1]</span></span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建训练集对象</span></span><br><span class="line">trainset = torchvision.datasets.CIFAR10(</span><br><span class="line">    root=<span class="string">&#x27;/path/to/data&#x27;</span>,     <span class="comment"># 数据存储路径</span></span><br><span class="line">    train=<span class="literal">True</span>,               <span class="comment"># 表示这是训练集</span></span><br><span class="line">    download=<span class="literal">True</span>,            <span class="comment"># 如果本地没有数据，则自动下载</span></span><br><span class="line">    transform=transform       <span class="comment"># 应用上面定义的图像预处理操作</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建训练集的数据加载器 DataLoader</span></span><br><span class="line">trainloader = torch.utils.data.DataLoader(</span><br><span class="line">    trainset,                  <span class="comment"># 使用的数据集</span></span><br><span class="line">    batch_size=<span class="number">4</span>,              <span class="comment"># 每个批次包含4个样本</span></span><br><span class="line">    shuffle=<span class="literal">True</span>,              <span class="comment"># 每个 epoch 开始前打乱数据顺序，提高泛化能力</span></span><br><span class="line">    num_workers=<span class="number">2</span>              <span class="comment"># 使用2个子进程加载数据，加快数据读取速度</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建测试集对象</span></span><br><span class="line">testset = torchvision.datasets.CIFAR10(</span><br><span class="line">    root=<span class="string">&#x27;/path/to/data&#x27;</span>,      <span class="comment"># 数据存储路径</span></span><br><span class="line">    train=<span class="literal">False</span>,               <span class="comment"># 表示这是测试集</span></span><br><span class="line">    download=<span class="literal">True</span>,             <span class="comment"># 如果本地没有数据，则自动下载</span></span><br><span class="line">    transform=transform        <span class="comment"># 同样应用预处理操作</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建测试集的数据加载器 DataLoader</span></span><br><span class="line">testloader = torch.utils.data.DataLoader(</span><br><span class="line">    testset,                   <span class="comment"># 使用的数据集</span></span><br><span class="line">    batch_size=<span class="number">4</span>,              <span class="comment"># 每个批次包含4个样本</span></span><br><span class="line">    shuffle=<span class="literal">False</span>,             <span class="comment"># 测试时不打乱数据，方便评估模型</span></span><br><span class="line">    num_workers=<span class="number">2</span>              <span class="comment"># 同样使用2个子进程加速数据加载</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># CIFAR-10 数据集中10个类别的标签名称（用于后续可视化或打印预测结果）</span></span><br><span class="line">classes = (<span class="string">&#x27;plane&#x27;</span>, <span class="string">&#x27;car&#x27;</span>, <span class="string">&#x27;bird&#x27;</span>, <span class="string">&#x27;cat&#x27;</span>,</span><br><span class="line">           <span class="string">&#x27;deer&#x27;</span>, <span class="string">&#x27;dog&#x27;</span>, <span class="string">&#x27;frog&#x27;</span>, <span class="string">&#x27;horse&#x27;</span>, <span class="string">&#x27;ship&#x27;</span>, <span class="string">&#x27;truck&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="3-4-一个完整的训练-推理流程"><a href="#3-4-一个完整的训练-推理流程" class="headerlink" title="3.4 一个完整的训练+推理流程"></a>3.4 一个完整的训练+推理流程</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(nn.Module):</span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="built_in">super</span>(Net, <span class="variable language_">self</span>).__init__()</span><br><span class="line">    <span class="variable language_">self</span>.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line">    <span class="variable language_">self</span>.pool = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">    <span class="variable language_">self</span>.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">    <span class="variable language_">self</span>.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)</span><br><span class="line">    <span class="variable language_">self</span>.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">    <span class="variable language_">self</span>.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">    x = <span class="variable language_">self</span>.pool(F.relu(<span class="variable language_">self</span>.conv1(x)))</span><br><span class="line">    x = <span class="variable language_">self</span>.pool(F.relu(<span class="variable language_">self</span>.conv2(x)))</span><br><span class="line">    x = x.view(-<span class="number">1</span>, <span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>)</span><br><span class="line">    x = F.relu(<span class="variable language_">self</span>.fc1(x))</span><br><span class="line">    x = F.relu(<span class="variable language_">self</span>.fc2(x))</span><br><span class="line">    x = <span class="variable language_">self</span>.fc3(x)</span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">net = Net()</span><br></pre></td></tr></table></figure>

<p>使用交叉熵损失函数，Optimizer使用带冲量的SGD。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure>

<p>遍历DataLoader进行训练。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):  <span class="comment"># 这里只迭代2个epoch，实际应该进行更多次训练</span></span><br><span class="line"></span><br><span class="line">  running_loss = <span class="number">0.0</span></span><br><span class="line">  <span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(trainloader, <span class="number">0</span>):</span><br><span class="line">    <span class="comment"># 得到输入</span></span><br><span class="line">    inputs, labels = data</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 梯度清零</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># forward + backward + optimize</span></span><br><span class="line">    outputs = net(inputs)</span><br><span class="line">    loss = criterion(outputs, labels)</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义统计信息</span></span><br><span class="line">    running_loss += loss.item()</span><br><span class="line">    <span class="keyword">if</span> i % <span class="number">2000</span> == <span class="number">1999</span>:</span><br><span class="line">      <span class="built_in">print</span>(<span class="string">&#x27;[%d, %5d] loss: %.3f&#x27;</span> %</span><br><span class="line">        (epoch + <span class="number">1</span>, i + <span class="number">1</span>, running_loss / <span class="number">2000</span>))</span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Finished Training&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>在测试集上进行测试。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">correct = <span class="number">0</span></span><br><span class="line">total = <span class="number">0</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> testloader:</span><br><span class="line">  images, labels = data</span><br><span class="line">  outputs = net(images)</span><br><span class="line">  _, predicted = torch.<span class="built_in">max</span>(outputs.data, <span class="number">1</span>)</span><br><span class="line">  total += labels.size(<span class="number">0</span>)</span><br><span class="line">  correct += (predicted == labels).<span class="built_in">sum</span>().item()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Accuracy of the network on the 10000 test images: %d %%&#x27;</span> % (</span><br><span class="line">  <span class="number">100</span> * correct / total))</span><br></pre></td></tr></table></figure>

<h3 id="3-5-在GPU上训练"><a href="#3-5-在GPU上训练" class="headerlink" title="3.5 在GPU上训练"></a>3.5 在GPU上训练</h3><p>使用 <code>torch.device</code> 将Tensor移到GPU上</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(device)</span><br></pre></td></tr></table></figure>

<p>使用GPU进行训练</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Net2</span>(nn.Module):</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line"><span class="built_in">super</span>(Net2, <span class="variable language_">self</span>).__init__()</span><br><span class="line"><span class="variable language_">self</span>.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">6</span>, <span class="number">5</span>).to(device)</span><br><span class="line"><span class="variable language_">self</span>.pool = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>).to(device)</span><br><span class="line"><span class="variable language_">self</span>.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>).to(device)</span><br><span class="line"><span class="variable language_">self</span>.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>).to(device)</span><br><span class="line"><span class="variable language_">self</span>.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>).to(device)</span><br><span class="line"><span class="variable language_">self</span>.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>).to(device)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">x = <span class="variable language_">self</span>.pool(F.relu(<span class="variable language_">self</span>.conv1(x)))</span><br><span class="line">x = <span class="variable language_">self</span>.pool(F.relu(<span class="variable language_">self</span>.conv2(x)))</span><br><span class="line">x = x.view(-<span class="number">1</span>, <span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>)</span><br><span class="line">x = F.relu(<span class="variable language_">self</span>.fc1(x))</span><br><span class="line">x = F.relu(<span class="variable language_">self</span>.fc2(x))</span><br><span class="line">x = <span class="variable language_">self</span>.fc3(x)</span><br><span class="line"><span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">net = Net2()</span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>):</span><br><span class="line"></span><br><span class="line">  running_loss = <span class="number">0.0</span></span><br><span class="line">  <span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(trainloader, <span class="number">0</span>):</span><br><span class="line">    <span class="comment"># 得到输入</span></span><br><span class="line">    inputs, labels = data</span><br><span class="line">    inputs, labels = inputs.to(device), labels.to(device)</span><br><span class="line">    <span class="comment"># 梯度清零</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># forward + backward + optimize</span></span><br><span class="line">    outputs = net(inputs)</span><br><span class="line">    loss = criterion(outputs, labels)</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义统计信息</span></span><br><span class="line">    running_loss += loss.item()</span><br><span class="line">    <span class="keyword">if</span> i % <span class="number">2000</span> == <span class="number">1999</span>:</span><br><span class="line">      <span class="built_in">print</span>(<span class="string">&#x27;[%d, %5d] loss: %.3f&#x27;</span> %</span><br><span class="line">        (epoch + <span class="number">1</span>, i + <span class="number">1</span>, running_loss / <span class="number">2000</span>))</span><br><span class="line">      running_loss = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Finished Training&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="3-6-使用autograd来实现三层神经网络"><a href="#3-6-使用autograd来实现三层神经网络" class="headerlink" title="3.6 使用autograd来实现三层神经网络"></a>3.6 使用autograd来实现三层神经网络</h3><p>实现一个全连接网络，只有一个隐层而且没有bias，使用欧氏距离作为损失函数。使用PyTorch的Tensor来计算前向阶段，然后使用PyTorch的autograd来自动帮我们反向计算梯度。PyTorch的Tensor代表了计算图中的一个节点。如果x是一个Tensor并且x.requires_grad&#x3D;True，那么x.grad这个Tensor会保存某个scalar(通常是loss)对x的梯度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">dtype = torch.<span class="built_in">float</span></span><br><span class="line">device = torch.device(<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># N是batch size；D_in是输入大小</span></span><br><span class="line"><span class="comment"># H是隐层的大小；D_out是输出大小</span></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建随机的Tensor作为输入和输出</span></span><br><span class="line"><span class="comment"># 输入和输出需要的requires_grad=False(默认)</span></span><br><span class="line">x = torch.randn(N, D_in, device=device, dtype=dtype)</span><br><span class="line">y = torch.randn(N, D_out, device=device, dtype=dtype)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建weight的Tensor，需要设置requires_grad=True</span></span><br><span class="line">w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=<span class="literal">True</span>)</span><br><span class="line">w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-6</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">500</span>):</span><br><span class="line">  <span class="comment"># Forward阶段: mm实现矩阵乘法，但是它不支持broadcasting。</span></span><br><span class="line">  <span class="comment"># 如果需要broadcasting，可以使用matmul</span></span><br><span class="line">  <span class="comment"># clamp本来的用途是把值clamp到指定的范围，这里实现ReLU。</span></span><br><span class="line">  y_pred = x.mm(w1).clamp(<span class="built_in">min</span>=<span class="number">0</span>).mm(w2)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># pow(2)实现平方计算。</span></span><br><span class="line">  <span class="comment"># loss.item()得到这个tensor的值。也可以直接打印loss，这会打印很多附加信息。</span></span><br><span class="line">  loss = (y_pred - y).<span class="built_in">pow</span>(<span class="number">2</span>).<span class="built_in">sum</span>()</span><br><span class="line">  <span class="built_in">print</span>(t, loss.item())</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 使用autograd进行反向计算。它会计算loss对所有对它有影响的</span></span><br><span class="line">  <span class="comment"># requires_grad=True的Tensor的梯度。</span></span><br><span class="line"></span><br><span class="line">  loss.backward()</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 手动使用梯度下降更新参数。一定要把更新的代码放到torch.no_grad()里</span></span><br><span class="line">  <span class="comment"># 否则下面的更新也会计算梯度。后面我们会使用torch.optim.SGD，</span></span><br><span class="line">  <span class="comment"># 它会帮我们管理这些用于更新梯度的计算。</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    w1 -= learning_rate * w1.grad</span><br><span class="line">    w2 -= learning_rate * w2.grad</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 手动把梯度清零</span></span><br><span class="line">    w1.grad.zero_()</span><br><span class="line">    w2.grad.zero_()</span><br></pre></td></tr></table></figure>

<h2 id="4-迁移学习"><a href="#4-迁移学习" class="headerlink" title="4. 迁移学习"></a><strong>4. 迁移学习</strong></h2><p>一般来说我们的训练数据量不会很大，很难达到像ImageNet那样上百万的标注数据集。我们可以使用迁移学习来解决训练数据不足的问题。迁移学习里，我们根据训练数据的多少通常可以采取如下方法：</p>
<ul>
<li><p>训练数据很少</p>
<p>那么我们通常把一个pretraning的网络的大部分固定住，然后只是把最后一个全连接层换成新的(最后一层通常是不一样的，因为分类的数量不同)，然后只训练这一层</p>
</li>
<li><p>训练数据较多</p>
<p>我们可以把pretraining的网络的前面一些层固定住，但后面的层不固定，把最后一层换新的，然后训练</p>
</li>
<li><p>训练数据很多</p>
<p>所有的pretraining的层都可以fine-tuning，只是用pretraining的参数作为初始化参数。</p>
</li>
</ul>
<h3 id="4-1-引入依赖"><a href="#4-1-引入依赖" class="headerlink" title="4.1 引入依赖"></a>4.1 引入依赖</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function, division</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> lr_scheduler</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, models, transforms</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"></span><br><span class="line">plt.ion()</span><br></pre></td></tr></table></figure>

<h3 id="4-2-加载数据"><a href="#4-2-加载数据" class="headerlink" title="4.2 加载数据"></a>4.2 加载数据</h3><p>使用torchvision和torch.utils.data包来加载数据。我们要解决的问题是训练一个模型来区分蚂蚁和蜜蜂，每个类别我们大概有120个训练数据，另外每个类有75个验证数据。这是一个很小的训练集，如果直接用一个神经网络来训练，效果会很差。现在我们用迁移学习来解决这个问题。数据可以在<a target="_blank" rel="noopener" href="https://download.pytorch.org/tutorial/hymenoptera_data.zip">这里</a>下载，下载后请解压到data目录下。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 训练时会做数据增强和归一化</span></span><br><span class="line"><span class="comment"># 验证的时候只做归一化</span></span><br><span class="line">data_transforms = &#123;</span><br><span class="line">  <span class="string">&#x27;train&#x27;</span>: transforms.Compose([</span><br><span class="line">    transforms.RandomResizedCrop(<span class="number">224</span>),</span><br><span class="line">    transforms.RandomHorizontalFlip(),</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">  ]),</span><br><span class="line">  <span class="string">&#x27;val&#x27;</span>: transforms.Compose([</span><br><span class="line">    transforms.Resize(<span class="number">256</span>),</span><br><span class="line">    transforms.CenterCrop(<span class="number">224</span>),</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line">  ]),</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">data_dir = <span class="string">&#x27;../data/hymenoptera_data&#x27;</span></span><br><span class="line">image_datasets = &#123;x: datasets.ImageFolder(os.path.join(data_dir, x),</span><br><span class="line">    data_transforms[x])</span><br><span class="line">  <span class="keyword">for</span> x <span class="keyword">in</span> [<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;val&#x27;</span>]&#125;</span><br><span class="line">dataloaders = &#123;x: torch.utils.data.DataLoader(image_datasets[x], batch_size=<span class="number">4</span>,</span><br><span class="line">    shuffle=<span class="literal">True</span>, num_workers=<span class="number">4</span>)</span><br><span class="line">  <span class="keyword">for</span> x <span class="keyword">in</span> [<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;val&#x27;</span>]&#125;</span><br><span class="line">dataset_sizes = &#123;x: <span class="built_in">len</span>(image_datasets[x]) <span class="keyword">for</span> x <span class="keyword">in</span> [<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;val&#x27;</span>]&#125;</span><br><span class="line">class_names = image_datasets[<span class="string">&#x27;train&#x27;</span>].classes</span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="4-3-训练模型"><a href="#4-3-训练模型" class="headerlink" title="4.3 训练模型"></a>4.3 训练模型</h3><p>在此实现一个用于训练模型的通用函数，包括：</p>
<ul>
<li>learning rate的自适应</li>
<li>保存最好的模型</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_model</span>(<span class="params">model, criterion, optimizer, shceduler, num_epochs=<span class="number">25</span></span>):</span><br><span class="line">				since = time.time()</span><br><span class="line">				</span><br><span class="line">				best_model_wts = copy.deepcopy(model.state_dict())</span><br><span class="line">				best_acc = <span class="number">0.0</span></span><br><span class="line">				</span><br><span class="line">				<span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">								<span class="built_in">print</span>(<span class="string">&#x27;Epoch &#123;&#125;/&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(epoch, num_epochs - <span class="number">1</span>))</span><br><span class="line">								<span class="built_in">print</span>(<span class="string">&#x27;-&#x27;</span> * <span class="number">10</span>)</span><br><span class="line">								</span><br><span class="line">								<span class="comment"># 每个epoch都分为训练和验证阶段</span></span><br><span class="line">								<span class="keyword">for</span> phase <span class="keyword">in</span> [<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;val&#x27;</span>]:</span><br><span class="line">												<span class="keyword">if</span> phase == <span class="string">&#x27;train&#x27;</span>:</span><br><span class="line">																scheduler.step()</span><br><span class="line">																model.train()   <span class="comment"># 训练阶段</span></span><br><span class="line">												<span class="keyword">else</span>:</span><br><span class="line">																model.<span class="built_in">eval</span>()    <span class="comment">#验证阶段</span></span><br><span class="line">																</span><br><span class="line">												running_loss = <span class="number">0.0</span></span><br><span class="line">												running_corrects = <span class="number">0</span></span><br><span class="line">												</span><br><span class="line">												<span class="comment"># 变量数据集</span></span><br><span class="line">												<span class="keyword">for</span> inputs, labels <span class="keyword">in</span> dataloaders[phase]:</span><br><span class="line">																inputs = inputs.to(device)</span><br><span class="line">																labels = labels.to(device)</span><br><span class="line">																</span><br><span class="line">												<span class="comment"># 清空参数梯度</span></span><br><span class="line">												optimizer.zero_grad()</span><br><span class="line">												</span><br><span class="line">												<span class="comment"># forward</span></span><br><span class="line">												<span class="comment"># 只有训练的时候track用于梯度计算的历史信息</span></span><br><span class="line">												<span class="keyword">with</span> torch.set_grad_enable(phase == <span class="string">&#x27;train&#x27;</span>):</span><br><span class="line">																outputs = model(inputs)</span><br><span class="line">																_, preds = torch.<span class="built_in">max</span>(outputs, <span class="number">1</span>)</span><br><span class="line">																loss = criterion(outputs, labels)</span><br><span class="line">																</span><br><span class="line">																<span class="comment"># 如果是训练，需要backward和更新参数</span></span><br><span class="line">																<span class="keyword">if</span> phase == <span class="string">&#x27;train&#x27;</span>:</span><br><span class="line">																				loss.backward()</span><br><span class="line">																				optimizer.step()</span><br><span class="line">																				</span><br><span class="line">												runnning_loss += loss.item() * inputs.size(<span class="number">0</span>)</span><br><span class="line">												running_corrects += torch.<span class="built_in">sum</span>(preds == labels.data)</span><br><span class="line">												</span><br><span class="line">												epoch_loss = running_loss / dataset_sizes[phase]</span><br><span class="line">												epoch_corrects = running_corrects.double() / dataset_sizes[phase]</span><br><span class="line">												</span><br><span class="line">												<span class="built_in">print</span>(<span class="string">&#x27;&#123;&#125; Loss: &#123;:.4f&#125; Acc: &#123;:.4f&#125;&#x27;</span>.<span class="built_in">format</span>(</span><br><span class="line">																phase, epoch_loss, epoch_acc))</span><br><span class="line">																</span><br><span class="line">												<span class="comment"># 保存验证集上的最佳模型</span></span><br><span class="line">												<span class="keyword">if</span> phase == <span class="string">&#x27;val&#x27;</span> <span class="keyword">and</span> epoch_acc &gt; best_acc:</span><br><span class="line">																best_acc = epoch_acc</span><br><span class="line">																best_model_wts = copy.deepcopy(model.state_dict())</span><br><span class="line">												<span class="built_in">print</span>()</span><br><span class="line">								</span><br><span class="line">				time_elapsed = time.time() - since</span><br><span class="line">				<span class="built_in">print</span>(<span class="string">&#x27;Training complete in &#123;:.0f&#125;m &#123;:.0f&#125;s&#x27;</span>.<span class="built_in">format</span>(</span><br><span class="line">time_elapsed // <span class="number">60</span>, time_elapsed % <span class="number">60</span>))</span><br><span class="line">				<span class="built_in">print</span>(<span class="string">&#x27;Best val Acc: &#123;:4f&#125;&#x27;</span>.<span class="built_in">format</span>(best_acc))</span><br><span class="line">				</span><br><span class="line">				<span class="comment"># 加载最优模型</span></span><br><span class="line">				model.load_state_dict(best_model_wts)</span><br><span class="line">				<span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>

<h3 id="4-4-可视化预测结果的函数"><a href="#4-4-可视化预测结果的函数" class="headerlink" title="4.4 可视化预测结果的函数"></a>4.4 可视化预测结果的函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">visualize_model</span>(<span class="params">model, num_images=<span class="number">6</span></span>):</span><br><span class="line">				was_training = model.training</span><br><span class="line">				model.<span class="built_in">eval</span>()</span><br><span class="line">				images_so_far = <span class="number">0</span></span><br><span class="line">				fig = plt.figure()</span><br><span class="line">				</span><br><span class="line">				<span class="keyword">with</span> torch.no_grad():</span><br><span class="line">								<span class="keyword">for</span> i, (inputs, labels) <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataloaders[<span class="string">&#x27;val&#x27;</span>]):</span><br><span class="line">												inputs = inputs.to(device)</span><br><span class="line">												labels = labels.to(device)</span><br><span class="line">												</span><br><span class="line">												outputs = model(inputs)</span><br><span class="line">												_, preds = torch.<span class="built_in">max</span>(outputs, <span class="number">1</span>)</span><br><span class="line">												</span><br><span class="line">												<span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(inputs.size()[<span class="number">0</span>]):</span><br><span class="line">																images_so_far += <span class="number">1</span></span><br><span class="line">																ax = plt.subplot(num_images//<span class="number">2</span>, <span class="number">2</span>, images_so_far)</span><br><span class="line">																ax.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">																ax.set_title(<span class="string">&#x27;predicted: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(class_names[preds[j]]))</span><br><span class="line">																imshow(inputs.cpu().data[j])</span><br><span class="line">																</span><br><span class="line">																<span class="keyword">if</span> images_so_far == num_images:</span><br><span class="line">																				model.train(mode=was_training)</span><br><span class="line">																				<span class="keyword">return</span></span><br><span class="line">								model.train(mode=was_training)</span><br></pre></td></tr></table></figure>

<h3 id="4-5-fine-tuning-所有参数"><a href="#4-5-fine-tuning-所有参数" class="headerlink" title="4.5 fine-tuning 所有参数"></a>4.5 fine-tuning 所有参数</h3><p>首先加载一个预训练的模型(imagenet上的resnet)，因为我们的类别数和imagenet不同，所以我们需要删掉原来的全连接层，换成新的全连接层。这里我们让所有的模型参数都可以调整，包括新加的全连接层和预训练的层。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">model_ft = models.resent18(pretrained=<span class="literal">True</span>) <span class="comment"># 加载预训练模型</span></span><br><span class="line">num_ftrs = model_ft.fc.in_features <span class="comment"># 获取最后一层输入特征值</span></span><br><span class="line">model_ft.fc = nn.Linear(num_ftrs, <span class="number">2</span>) <span class="comment"># 替换最后一层全连接层</span></span><br><span class="line"></span><br><span class="line">model_ft = model_ft.to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 所有参数都可以训练</span></span><br><span class="line">optimizer_ft = optim.SGD(model_ft.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 每7个epoch learning rate变为原来的10% </span></span><br><span class="line">exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=<span class="number">7</span>, gamma=<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line">model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,</span><br><span class="line">	num_epochs=<span class="number">25</span>)</span><br></pre></td></tr></table></figure>

<h3 id="4-6-fine-tuning-最后一层参数"><a href="#4-6-fine-tuning-最后一层参数" class="headerlink" title="4.6 fine-tuning 最后一层参数"></a>4.6 fine-tuning 最后一层参数</h3><p>我们用可以固定住前面层的参数，只训练最后一层。这比之前要快将近一倍，因为反向计算梯度只需要计算最后一层。但是前向计算的时间是一样的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">model_conv = torchvision.models.resnet18(pretrained=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 冻结所有参数</span></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> model_conv.parameters():</span><br><span class="line">	param.requires_grad = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 新加的层默认requires_grad=True </span></span><br><span class="line">num_ftrs = model_conv.fc.in_features</span><br><span class="line">model_conv.fc = nn.Linear(num_ftrs, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">model_conv = model_conv.to(device)</span><br><span class="line"></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 值训练最后一个全连接层。</span></span><br><span class="line">optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line">exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=<span class="number">7</span>, gamma=<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line">model_conv = train_model(model_conv, criterion, optimizer_conv,</span><br><span class="line">	exp_lr_scheduler, num_epochs=<span class="number">25</span>)</span><br></pre></td></tr></table></figure>

    </div>

    
    <div class="about">
        <h1>About this Post</h1>
        <div class="details">
            <p>This post is written by Latteo, licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc/4.0">CC BY-NC 4.0</a>.</p>
        </div>
        
    </div>
    

    <div class="container post-prev-next">
        
        <a href="/2025/07/04/Docker-%E7%AE%80%E6%98%93%E6%95%99%E7%A8%8B/" class="next">
            <div>
                <div class="text">
                    <p class="label">Next</p>
                    <h3 class="title">Docker 简易教程</h3>
                </div>
            </div>
        </a>
        
        
        <a href="/2025/06/24/start-my-blog/" class="prev">
            <div>
                <div class="text">
                    <p class="label">Previous</p>
                    <h3 class="title">start_my_blog</>
                </div>
            </div>
        </a>
        
    </div>

    
        
        
    
</article>

        <footer>
    <div class="inner">
        <div class="links">
            
        </div>
        <span>&copy; 2025 Latteo<br>Powered by <a href="http://hexo.io/" target="_blank">Hexo</a> </span>
        
        
            <br>
            <div class="color-scheme-toggle" role="radiogroup" id="theme-color-scheme-toggle">
                <label>
                    <input type="radio" value="light">
                    <span>Light</span>
                </label>
                <label>
                    <input type="radio" value="dark">
                    <span>Dark</span>
                </label>
                <label>
                    <input type="radio" value="auto">
                    <span>Auto</span>
                </label>
            </div>
        
    </div>
</footer>


        
<script src="/js/main.js"></script>

        
        
        

        
        <script src="https://unpkg.com/scrollreveal"></script>
        <script>
            window.addEventListener('load', () => {
                ScrollReveal({ delay: 250, reset: true, easing: 'cubic-bezier(0, 0, 0, 1)' })
                ScrollReveal().reveal('.post-list-item .cover-img img')
                ScrollReveal().reveal('.post-list-item, .card, .content p img, .content .block-large img', { distance: '60px', origin: 'bottom', duration: 800 })
            })
        </script>
        
    </body>
</html>