<!DOCTYPE html>
<html lang="en">
    <head prefix="og: https://ogp.me/ns#">
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="color-scheme" content="light dark">
  
  <title>pytorch 简易入门 - Hexo</title>
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    <link rel='manifest' href='/manifest.json'>
  

  
  
  
  <meta property="og:title" content="pytorch 简易入门 - Hexo" />
  
  <meta property="og:type" content="article" />
  
  <meta property="og:url" content="http://example.com/2025/06/29/pytorch/index.html" />
  
  <meta property="og:image" content="/favicon.png" />
  
  <meta property="og:article:published_time" content="2025-06-29T13:08:08.000Z" />
  
  <meta property="og:article:author" content="Latteo" />
  
  

  
<link rel="stylesheet" href="/css/var.css">

  
<link rel="stylesheet" href="/css/main.css">

  
<link rel="stylesheet" href="/css/typography.css">

  
<link rel="stylesheet" href="/css/code-highlighting.css">

  
<link rel="stylesheet" href="/css/components.css">

  
<link rel="stylesheet" href="/css/nav.css">

  
<link rel="stylesheet" href="/css/paginator.css">

  
<link rel="stylesheet" href="/css/footer.css">

  
<link rel="stylesheet" href="/css/post-list.css">

  
  
  
<link rel="stylesheet" href="/css/toc.css">

  
  
  
  
  
<link rel="stylesheet" href="/css/post.css">

  
  
  
  
  
  
<link rel="stylesheet" href="/css/main.css">

  
<link rel="stylesheet" href="/css/nav.css">

  
<link rel="stylesheet" href="/css/components.css">

  
<link rel="stylesheet" href="/css/categories.css">

  
<link rel="stylesheet" href="/css/tags.css">

  
<link rel="stylesheet" href="/css/archive.css">

  
<link rel="stylesheet" href="/css/post.css">

  
<link rel="stylesheet" href="/css/code-highlighting.css">

  
<link rel="stylesheet" href="/css/giscus.css">


    

    

  
<meta name="generator" content="Hexo 7.3.0"></head>
    <body
        data-color-scheme="auto"
        data-uppercase-categories="true"
        
        data-config-root="/"
        
        data-toc="true"
        data-toc-max-depth="2"
        
        
    >
        <nav id="theme-nav">
    <div class="inner">
        <a class="title" href="/">Blog</a>
        <div class="nav-arrow"></div>
        <div class="nav-items">
            <a class="nav-item nav-item-home" href="/">Home</a>
            
            
            <a class="nav-item" href="/archives">Archives</a>
            
            
            
            <a class="nav-item" href="/projects">Projects</a>
            
            
            
            <a class="nav-item" href="/about">About</a>
            
            
            
            <a class="nav-item nav-item-github nav-item-icon" href="https://github.com/c0ffee-milk" target="_blank" aria-label="GitHub">&nbsp;</a>
            
            
            
            <a class="nav-item nav-item-search nav-item-icon" href="/search" target="_blank" aria-label="Search">&nbsp;</a>
            
            
        </div>
    </div>
</nav>
        
<article class="post">
    <div class="meta">
        

        
        <div class="date" id="date">
            <span>June</span>
            <span>29,</span>
            <span>2025</span>
        </div>
        

        <h1 class="title">pytorch 简易入门</h1>
    </div>

    <div class="divider"></div>

    <div class="content">
        <h2 id="1-PyTorch-的基本概念"><a href="#1-PyTorch-的基本概念" class="headerlink" title="1. PyTorch 的基本概念"></a><strong>1. PyTorch 的基本概念</strong></h2><h3 id="1-1-Tensor"><a href="#1-1-Tensor" class="headerlink" title="1.1 Tensor"></a>1.1 Tensor</h3><p>Tensor类似与NumPy的ndarray，但是可以用GPU加速。</p>
<ol>
<li><p>构造未初始化矩阵 <code>empty</code></p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">x = torch.empty(5, 3)</span><br><span class="line">print(x)</span><br><span class="line"></span><br><span class="line">tensor([[0., 0., 0.],</span><br><span class="line">        [0., 0., 0.],</span><br><span class="line">        [0., 0., 0.],</span><br><span class="line">        [0., 0., 0.],</span><br><span class="line">        [0., 0., 0.]])</span><br></pre></td></tr></table></figure>
</li>
<li><p>随机初始化一个矩阵 <code>rand</code></p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand(5, 3)</span><br><span class="line">print(x)</span><br><span class="line"></span><br><span class="line">tensor([[0.6819, 0.1574, 0.6951],</span><br><span class="line">        [0.9938, 0.7842, 0.3295],</span><br><span class="line">        [0.6799, 0.4662, 0.9871],</span><br><span class="line">        [0.1258, 0.4621, 0.7077],</span><br><span class="line">        [0.3343, 0.1081, 0.6953]])</span><br></pre></td></tr></table></figure>
</li>
<li><p>构造一个零初始化矩阵 <code>zeros</code></p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">x = torch.zeros(5, 3)</span><br><span class="line">print(x)</span><br><span class="line"></span><br><span class="line">tensor([[0., 0., 0.],</span><br><span class="line">        [0., 0., 0.],</span><br><span class="line">        [0., 0., 0.],</span><br><span class="line">        [0., 0., 0.],</span><br><span class="line">        [0., 0., 0.]])</span><br></pre></td></tr></table></figure>
</li>
<li><p>构造一个指定数据类型的矩阵 <code>dtype</code></p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand(5, 3, dtype=torch.float32)</span><br><span class="line">print(x)</span><br><span class="line"></span><br><span class="line">tensor([[0.9735, 0.6720, 0.1250],</span><br><span class="line">        [0.3461, 0.6952, 0.1919],</span><br><span class="line">        [0.7887, 0.4517, 0.9117],</span><br><span class="line">        [0.5153, 0.8150, 0.3046],</span><br><span class="line">        [0.5150, 0.4235, 0.8831]])</span><br></pre></td></tr></table></figure>
</li>
<li><p>使用数组构造Tensor</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([5.5, 3])</span><br><span class="line">print(x)</span><br><span class="line"></span><br><span class="line">tensor([5.5000, 3.0000])</span><br></pre></td></tr></table></figure>
</li>
<li><p>查看一个Tensor的shape <code>size</code></p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">x = torch.zeros(5, 3)</span><br><span class="line">print(x)</span><br><span class="line">print(x.size())</span><br><span class="line"></span><br><span class="line">tensor([[0., 0., 0.],</span><br><span class="line">        [0., 0., 0.],</span><br><span class="line">        [0., 0., 0.],</span><br><span class="line">        [0., 0., 0.],</span><br><span class="line">        [0., 0., 0.]])</span><br><span class="line">torch.Size([5, 3])</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="1-2-Operation"><a href="#1-2-Operation" class="headerlink" title="1.2 Operation"></a>1.2 Operation</h3><p>Operation一般可以使用函数的方式使用，以加法为例</p>
<ol>
<li><p>用运算符实现加法 <code>+</code></p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand(5, 3)</span><br><span class="line">y = torch.rand(5, 3)</span><br><span class="line">print(x)</span><br><span class="line">print(y)</span><br><span class="line">print(x + y)</span><br><span class="line"></span><br><span class="line">tensor([[0.8217, 0.7760, 0.0340],</span><br><span class="line">        [0.0413, 0.5867, 0.6914],</span><br><span class="line">        [0.4074, 0.2192, 0.8599],</span><br><span class="line">        [0.1837, 0.4944, 0.1833],</span><br><span class="line">        [0.9589, 0.1776, 0.0896]])</span><br><span class="line">tensor([[0.5083, 0.3603, 0.0964],</span><br><span class="line">        [0.5016, 0.6730, 0.5046],</span><br><span class="line">        [0.7970, 0.7357, 0.1990],</span><br><span class="line">        [0.2162, 0.4779, 0.5341],</span><br><span class="line">        [0.8595, 0.5517, 0.4641]])</span><br><span class="line">tensor([[1.3300, 1.1363, 0.1304],</span><br><span class="line">        [0.5429, 1.2597, 1.1960],</span><br><span class="line">        [1.2044, 0.9549, 1.0589],</span><br><span class="line">        [0.3999, 0.9722, 0.7175],</span><br><span class="line">        [1.8184, 0.7293, 0.5537]])</span><br></pre></td></tr></table></figure>
</li>
<li><p>用函数实现加法 <code>torch.add()</code></p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand(5, 3)</span><br><span class="line">y = torch.rand(5, 3)</span><br><span class="line">print(x)</span><br><span class="line">print(y)</span><br><span class="line">print(torch.add(x, y))</span><br><span class="line"></span><br><span class="line">tensor([[0.4643, 0.0701, 0.3483],</span><br><span class="line">        [0.1846, 0.7678, 0.8397],</span><br><span class="line">        [0.9759, 0.9102, 0.5844],</span><br><span class="line">        [0.2472, 0.2512, 0.6340],</span><br><span class="line">        [0.2352, 0.7352, 0.9725]])</span><br><span class="line">tensor([[0.0229, 0.0309, 0.7605],</span><br><span class="line">        [0.5113, 0.8178, 0.3730],</span><br><span class="line">        [0.8731, 0.6131, 0.5512],</span><br><span class="line">        [0.7176, 0.8111, 0.4682],</span><br><span class="line">        [0.5177, 0.2994, 0.3149]])</span><br><span class="line">tensor([[0.4872, 0.1010, 1.1088],</span><br><span class="line">        [0.6959, 1.5856, 1.2127],</span><br><span class="line">        [1.8490, 1.5233, 1.1356],</span><br><span class="line">        [0.9649, 1.0623, 1.1022],</span><br><span class="line">        [0.7528, 1.0346, 1.2874]])</span><br></pre></td></tr></table></figure>
</li>
<li><p>为加法提供返回值 <code>out=</code></p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand(5, 3)</span><br><span class="line">y = torch.rand(5, 3)</span><br><span class="line">result = torch.empty(5, 3)</span><br><span class="line">torch.add(x, y, out=result)</span><br><span class="line">print(result)</span><br><span class="line"></span><br><span class="line">tensor([[1.3815, 1.2649, 1.3772],</span><br><span class="line">        [0.8191, 1.0258, 0.7270],</span><br><span class="line">        [1.5077, 1.2752, 0.8296],</span><br><span class="line">        [0.8453, 1.0702, 1.3118],</span><br><span class="line">        [1.0655, 1.7370, 0.9018]])</span><br></pre></td></tr></table></figure>
</li>
<li><p>原地修改Tensor <code>add_(以下划线结尾)</code></p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand(5, 3)</span><br><span class="line">y = torch.rand(5, 3)</span><br><span class="line">print(x)</span><br><span class="line">print(y)</span><br><span class="line">print(y.add_(x))</span><br><span class="line"></span><br><span class="line">tensor([[0.1245, 0.0032, 0.2941],</span><br><span class="line">        [0.6233, 0.5174, 0.7957],</span><br><span class="line">        [0.6105, 0.4572, 0.8840],</span><br><span class="line">        [0.6559, 0.8040, 0.7680],</span><br><span class="line">        [0.0818, 0.1590, 0.2294]])</span><br><span class="line">tensor([[0.6524, 0.8919, 0.6358],</span><br><span class="line">        [0.8619, 0.3013, 0.1395],</span><br><span class="line">        [0.7151, 0.4323, 0.2184],</span><br><span class="line">        [0.5981, 0.9320, 0.9239],</span><br><span class="line">        [0.0647, 0.4407, 0.3301]])</span><br><span class="line">tensor([[0.7769, 0.8951, 0.9299],</span><br><span class="line">        [1.4853, 0.8186, 0.9352],</span><br><span class="line">        [1.3256, 0.8895, 1.1024],</span><br><span class="line">        [1.2540, 1.7360, 1.6919],</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="1-3-Tensor的变换"><a href="#1-3-Tensor的变换" class="headerlink" title="1.3 Tensor的变换"></a>1.3 Tensor的变换</h3><ol>
<li><p>resize和reshape一个Tensor <code>torch.view</code></p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand(4, 4)</span><br><span class="line">y = x.view(-1, 2) # -1表示让Pytorch自己推断该维的大小</span><br><span class="line">z = x.view(16)</span><br><span class="line">print(x)</span><br><span class="line">print(y)</span><br><span class="line">print(z)</span><br><span class="line"></span><br><span class="line">tensor([[0.7446, 0.6093, 0.6807, 0.3331],</span><br><span class="line">        [0.5358, 0.1826, 0.0263, 0.8574],</span><br><span class="line">        [0.6927, 0.6911, 0.7574, 0.4551],</span><br><span class="line">        [0.0486, 0.9530, 0.8304, 0.0535]])</span><br><span class="line">tensor([[0.7446, 0.6093],</span><br><span class="line">        [0.6807, 0.3331],</span><br><span class="line">        [0.5358, 0.1826],</span><br><span class="line">        [0.0263, 0.8574],</span><br><span class="line">        [0.6927, 0.6911],</span><br><span class="line">        [0.7574, 0.4551],</span><br><span class="line">        [0.0486, 0.9530],</span><br><span class="line">        [0.8304, 0.0535]])</span><br><span class="line">tensor([0.7446, 0.6093, 0.6807, 0.3331, 0.5358, 0.1826, 0.0263, 0.8574, 0.6927,</span><br><span class="line">        0.6911, 0.7574, 0.4551, 0.0486, 0.9530, 0.8304, 0.0535])</span><br></pre></td></tr></table></figure>
</li>
<li><p>将单个元素的Tensor转化为Python number <code>torch.item</code></p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(1)</span><br><span class="line">print(x)</span><br><span class="line">#输出的是一个Tensor</span><br><span class="line">tensor([-0.6966])</span><br><span class="line"></span><br><span class="line">print(x.item())</span><br><span class="line">#输出的是一个数</span><br><span class="line">-0.6966081857681274</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="1-4-Tensor-与-Numpy-的相互转换"><a href="#1-4-Tensor-与-Numpy-的相互转换" class="headerlink" title="1.4 Tensor 与 Numpy 的相互转换"></a>1.4 Tensor 与 Numpy 的相互转换</h3><p>Tensor 与 Numpy 共享内存地址，当其中一方被修改时也会影响另一方。</p>
<p>把NumPy数组转成Torch Tensor <code>torch.from_numpy()</code>：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">a = np.ones(5)</span><br><span class="line">b = torch.from_numpy(a)</span><br><span class="line">np.add(a, 1, out=a)</span><br><span class="line">print(a)</span><br><span class="line"># [2. 2. 2. 2. 2.]</span><br><span class="line">print(b)</span><br><span class="line"># tensor([ 2.,  2.,  2.,  2.,  2.], dtype=torch.float64)</span><br></pre></td></tr></table></figure>

<h3 id="1-5-CUDA-Tensor"><a href="#1-5-CUDA-Tensor" class="headerlink" title="1.5 CUDA Tensor"></a>1.5 CUDA Tensor</h3><p>Tensor可以使用to()方法来移到任意设备上</p>
<p>如果设备有cuda，使用 <code>torch.device</code>  将tensors放在GPU上</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand(4, 4)</span><br><span class="line">if torch.cuda.is_available():</span><br><span class="line">        device = torch.device(&quot;cuda&quot;)</span><br><span class="line">        y = torch.ones_like(x, device=device)</span><br><span class="line">        x = x.to(device)</span><br><span class="line">        z = x + y</span><br><span class="line">        print(z)</span><br><span class="line">        print(z.to(&quot;cpu&quot;, torch.double))</span><br></pre></td></tr></table></figure>

<h3 id="1-6-Autograd-自动求导"><a href="#1-6-Autograd-自动求导" class="headerlink" title="1.6 Autograd 自动求导"></a>1.6 Autograd 自动求导</h3><p>Autograd 包是 Pytorch 的核心，Autograd为所有用于Tensor的operation提供自动求导的功能。</p>
<ol>
<li>反向传播<br>torch.Tensor 是 Autograd包的核心类，若其属性requires_grad 为True，PyTorch就会追踪所有与之相关的operation。当完成(正向)计算之后， 我们可以调用backward()，PyTorch会自动的把所有的梯度都计算好。与这个tensor相关的梯度都会累加到它的grad属性里。</li>
<li>禁用反向传播<br>如果不想计算这个tensor的梯度，我们可以调用detach()，这样它就不会参与梯度的计算了。为了阻止PyTorch记录用于梯度计算相关的信息(从而节约内存)，我们可以使用 with torch.no_grad()。这在模型的推理时非常有用，因为推理的时候我们不需要计算梯度，否则我们就得一个个的修改Tensor的requires_grad属性，这会非常麻烦。</li>
<li>计算梯度<br>如果希望想计算梯度，可以对一个Tensor调用它的backward()方法。如果这个Tensor是一个scalar(只有一个数)，那么调用时不需要传任何参数。如果Tensor多于一个数，那么需要传入和它的shape一样的参数，表示反向传播过来的梯度。</li>
</ol>
<p>示例：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"># 创建tensor时设置属性requires_grad=True</span><br><span class="line">x = torch.ones(2, 2, requires_grad=True)</span><br><span class="line">print(x)</span><br><span class="line"></span><br><span class="line"># 通过Operation产生新的tensor</span><br><span class="line">y = x + 2</span><br><span class="line">print(y)</span><br><span class="line"></span><br><span class="line"># 通过Operation产生的tensor，其grad_fn不为None</span><br><span class="line">print(y.grad_fn)</span><br><span class="line"></span><br><span class="line">z = y * y * 3</span><br><span class="line">out = z.mean()</span><br><span class="line"></span><br><span class="line">print(z)</span><br><span class="line">print(out)</span><br><span class="line"></span><br><span class="line">tensor([[1., 1.],</span><br><span class="line">        [1., 1.]], requires_grad=True)</span><br><span class="line">tensor([[3., 3.],</span><br><span class="line">        [3., 3.]], grad_fn=&lt;AddBackward0&gt;)</span><br><span class="line">&lt;AddBackward0 object at 0x1007be670&gt;</span><br><span class="line">tensor([[27., 27.],</span><br><span class="line">        [27., 27.]], grad_fn=&lt;MulBackward0&gt;)</span><br><span class="line">tensor(27., grad_fn=&lt;MeanBackward0&gt;)</span><br></pre></td></tr></table></figure>

<p>使用 <code>requires_grad_()</code>  改变一个tensor的requires_grad</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(2, 2)</span><br><span class="line">a = ((a * 3) / (a - 1))</span><br><span class="line">print(a.requires_grad)</span><br><span class="line">a.requires_grad_(True)</span><br><span class="line">print(a.requires_grad)</span><br><span class="line">b = (a * a).sum()</span><br><span class="line">print(b.grad_fn)</span><br><span class="line"></span><br><span class="line">False</span><br><span class="line">True</span><br><span class="line">&lt;SumBackward0 object at 0x102b36670&gt;</span><br></pre></td></tr></table></figure>

<h3 id="1-7-梯度"><a href="#1-7-梯度" class="headerlink" title="1.7 梯度"></a>1.7 梯度</h3><p>使用 <code>backword()</code> 反向计算梯度</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">out.backward()</span><br><span class="line">print(x.grad)</span><br><span class="line"></span><br><span class="line">tensor([[4.5000, 4.5000],</span><br><span class="line">        [4.5000, 4.5000]])</span><br></pre></td></tr></table></figure>

<p>使用 <code>with torch.no_grad()</code> 停止计算梯度</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">x = torch.ones(2, 2, requires_grad=True)</span><br><span class="line">print(x.requires_grad)</span><br><span class="line">print((x ** 2).requires_grad)</span><br><span class="line"></span><br><span class="line">with torch.no_grad():</span><br><span class="line">  print((x ** 2).requires_grad)</span><br><span class="line"></span><br><span class="line">True</span><br><span class="line">True</span><br><span class="line">False</span><br></pre></td></tr></table></figure>

<h2 id="2-PyTorch-神经网络"><a href="#2-PyTorch-神经网络" class="headerlink" title="2. PyTorch 神经网络"></a><strong>2. PyTorch 神经网络</strong></h2><p>神经网络可以通过torch.nn包来创建。我们之前简单的了解了autograd，而nn会使用autograd来定义模型以及求梯度。一个nn.Module对象包括了许多网络层(layer)，并且有一个forward(input)方法来返回output。</p>
<p>下图是一个识别mnist图片的卷积网络结构：</p>
<p><img src="https://raw.githubusercontent.com/c0ffee-milk/photo/master/202506302313658.png" alt="mnist"></p>
<p>训练一个神经网络通常需要如下步骤：</p>
<ol>
<li><p>定义一个神经网络，它通常有一些可以训练的参数</p>
</li>
<li><p>迭代一个数据集(dataset)</p>
</li>
<li><p>处理网络的输入</p>
</li>
<li><p>计算loss(会调用Module对象的forward方法)</p>
</li>
<li><p>计算loss对参数的梯度</p>
</li>
<li><p>更新参数，通常使用如下的梯度下降方法来更新：</p>
<p> weight &#x3D; weight - learning_rate * gradient</p>
</li>
</ol>
<h3 id="2-1-定义网络"><a href="#2-1-定义网络" class="headerlink" title="2.1 定义网络"></a>2.1 定义网络</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line">import torch.nn.functional as F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Net(nn.Module):</span><br><span class="line">      def __init__(self):</span><br><span class="line">              super(Net, self).__init__()</span><br><span class="line">              # 第一层卷积层，输入1个通道（灰度图），输出6个通道（feature map），使用5*5的卷积核</span><br><span class="line">              self.conv1 = nn.Conv2d(1, 6, 5)</span><br><span class="line">              # 第二层卷积层，输入6个通道，输出16个通道，使用5*5的卷积核</span><br><span class="line">              self.conv2 = nn.Conv2d(6, 16, 5)</span><br><span class="line">              # 全连接层</span><br><span class="line">              self.fc1 = nn.Linear(16*5*5, 120)</span><br><span class="line">              self.fc2 = nn.Linear(120, 84)</span><br><span class="line">              self.fc3 = nn.Linear(84, 10)</span><br><span class="line"></span><br><span class="line">      # 前向传播</span><br><span class="line">      def forward(self, x):</span><br><span class="line">              # 第一层卷积+池化层 32x32 -&gt; 28x28 -&gt; 14x14</span><br><span class="line">              x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))</span><br><span class="line">              # 第二层卷积+池化层 14x14 -&gt; 10x10 -&gt; 5x5</span><br><span class="line">              x = F.max_pool2d(F.relu(self.conv2(x)), 2) # 2与（2， 2）效果相同</span><br><span class="line">              x = x.view(-1, self.num_flat_features(x)) # 将多维张量平铺为二维张量</span><br><span class="line">              x = F.relu(self.fc1(x))</span><br><span class="line">              x = F.relu(self.fc2(x))</span><br><span class="line">              x = self.fc3(x)</span><br><span class="line">              return x</span><br><span class="line"></span><br><span class="line">      def num_flat_features(self, x):</span><br><span class="line">              size = x.size()[1:]  # 除了batch维度之外的其它维度。</span><br><span class="line">              num_features = 1</span><br><span class="line">              for s in size:</span><br><span class="line">              num_features *= s</span><br><span class="line">              return num_features</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = Net()</span><br><span class="line">print(net)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Net(</span><br><span class="line">  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))</span><br><span class="line">  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))</span><br><span class="line">  (fc1): Linear(in_features=400, out_features=120, bias=True)</span><br><span class="line">  (fc2): Linear(in_features=120, out_features=84, bias=True)</span><br><span class="line">  (fc3): Linear(in_features=84, out_features=10, bias=True)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>我们只需要定义forward函数，而backward函数会自动通过autograd创建。在forward函数里可以使用任何处理Tensor的函数。我们可以使用函数net.parameters()来得到模型所有的参数。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">params = list(net.parameters())</span><br><span class="line">print(len(params))</span><br><span class="line"># 10</span><br><span class="line">print(params[0].size())  # conv1的weight</span><br><span class="line"># torch.Size([6, 1, 5, 5])</span><br></pre></td></tr></table></figure>

<h3 id="2-2-测试网络"><a href="#2-2-测试网络" class="headerlink" title="2.2 测试网络"></a>2.2 测试网络</h3><p>这个网络(LeNet)期望的输入大小是32x32。如果使用MNIST数据集(28x28)，我们需要缩放到32x32。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">input = torch.randn(1, 1, 32, 32)</span><br><span class="line">out = net(input)</span><br><span class="line">print(out)</span><br><span class="line"></span><br><span class="line">tensor([[-0.0279, -0.0748,  0.0013,  0.0537,  0.0150, -0.1020, 0.0109, -0.0757, -0.0951,  0.0703]], grad_fn=&lt;AddmmBackward0&gt;)</span><br></pre></td></tr></table></figure>

<p>默认的梯度会累加，因此我们通常在backward之前清除掉之前的梯度值：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net.zero_grad()</span><br><span class="line">out.backward(torch.randn(1, 10))</span><br></pre></td></tr></table></figure>

<h3 id="2-3-损失函数"><a href="#2-3-损失函数" class="headerlink" title="2.3 损失函数"></a>2.3 损失函数</h3><p>损失函数的参数是(output, target)对，output是模型的预测，target是实际的值。损失函数会计算预测值和真实值的差别，损失越小说明预测的越准。</p>
<p>最简单的一个损失函数是：nn.MSELoss，它会计算预测值和真实值的均方误差。比如：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">output = net(input)</span><br><span class="line">target = torch.arange(1, 11)</span><br><span class="line">target = target.view(1, -1)</span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line"></span><br><span class="line">loss = criterion(output, target)</span><br><span class="line">print(loss)</span><br></pre></td></tr></table></figure>

<h3 id="2-4-计算梯度"><a href="#2-4-计算梯度" class="headerlink" title="2.4 计算梯度"></a>2.4 计算梯度</h3><p>在调用loss.backward()之前，我们需要清除掉tensor里之前的梯度，否则会累加进去。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">net.zero_grad()     # 清掉tensor里缓存的梯度值。</span><br><span class="line"></span><br><span class="line">print(&#x27;conv1.bias.grad before backward&#x27;)</span><br><span class="line">print(net.conv1.bias.grad)</span><br><span class="line"></span><br><span class="line">loss.backward()</span><br><span class="line"></span><br><span class="line">print(&#x27;conv1.bias.grad after backward&#x27;)</span><br><span class="line">print(net.conv1.bias.grad)</span><br></pre></td></tr></table></figure>

<h3 id="2-5-更新参数"><a href="#2-5-更新参数" class="headerlink" title="2.5 更新参数"></a>2.5 更新参数</h3><p>更新参数最简单的方法是使用随机梯度下降(SGD)： 𝑤𝑒𝑖𝑔ℎ𝑡&#x3D;𝑤𝑒𝑖𝑔ℎ𝑡−𝑙𝑒𝑎𝑟𝑛𝑖𝑛𝑔𝑟𝑎𝑡𝑒∗𝑔𝑟𝑎𝑑𝑖𝑒𝑛𝑡</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">learning_rate = 0.01</span><br><span class="line">for f in net.parameters():</span><br><span class="line">  f.data.sub_(f.grad.data * learning_rate)</span><br></pre></td></tr></table></figure>

<p>通常我们会使用更加复杂的优化方法，比如SGD, Nesterov-SGD, Adam, RMSProp等等。为了实现这些算法，我们可以使用torch.optim包，它的用法也非常简单：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">import torch.optim as optim</span><br><span class="line"></span><br><span class="line"># 创建optimizer需要传入参数和learning rate</span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=0.01)</span><br><span class="line"></span><br><span class="line"># 清除梯度</span><br><span class="line">optimizer.zero_grad()</span><br><span class="line">output = net(input)</span><br><span class="line">loss = criterion(output, target)</span><br><span class="line">loss.backward()</span><br><span class="line">optimizer.step()    # optimizer会自动帮我们更新参数</span><br></pre></td></tr></table></figure>

<h2 id="3-训练一个分类器"><a href="#3-训练一个分类器" class="headerlink" title="3. 训练一个分类器"></a><strong>3. 训练一个分类器</strong></h2><h3 id="3-1-数据处理"><a href="#3-1-数据处理" class="headerlink" title="3.1 数据处理"></a>3.1 数据处理</h3><p>一般地，当我们处理图片、文本、音频或者视频数据的时候，我们可以使用python代码来把它转换成numpy数组。然后再把numpy数组转换成torch.xxxTensor。</p>
<p>CIFAR10数据集包括十个类别：”airplane”, “automobile”, “bird”, “cat”, “deer”, “dog”, “frog”, “horse”, “ship”,”truck”。图像的对象是3x32x32，也就是3通道(RGB)的32x32的图片。</p>
<p><img src="https://raw.githubusercontent.com/c0ffee-milk/photo/master/202506302313517.png" alt="cifar10"></p>
<h3 id="3-2-训练步骤"><a href="#3-2-训练步骤" class="headerlink" title="3.2 训练步骤"></a>3.2 训练步骤</h3><ol>
<li>使用torchvision加载和预处理CIFAR10训练和测试数据集。</li>
<li>定义卷积网络</li>
<li>定义损失函数</li>
<li>用训练数据训练模型</li>
<li>用测试数据测试模型</li>
</ol>
<h3 id="3-3-加载数据"><a href="#3-3-加载数据" class="headerlink" title="3.3 加载数据"></a>3.3 加载数据</h3><p>使用torchvision加载CIFAR10数据集：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torchvision</span><br><span class="line">import torchvision.transforms as transforms</span><br></pre></td></tr></table></figure>

<p>torchvision读取的datasets是PILImage对象，它的取值范围是[0, 1]，我们把它转换到范围[-1, 1]。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"># 导入必要的图像变换模块（transforms）和数据集模块（如 CIFAR10）</span><br><span class="line">import torchvision.transforms as transforms</span><br><span class="line">import torchvision</span><br><span class="line"></span><br><span class="line"># 定义图像预处理操作：将图像转换为张量，并进行标准化</span><br><span class="line">transform = transforms.Compose(</span><br><span class="line">    [</span><br><span class="line">        # 将 PIL 图像或 numpy 数组转换为 Tensor 张量</span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line"></span><br><span class="line">        # 对图像进行标准化处理：</span><br><span class="line">        # 每个通道减去均值，再除以标准差</span><br><span class="line">        # 这里使用的是 CIFAR-10 数据集的经验参数</span><br><span class="line">        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # 输入范围 [0,1] → 输出范围 [-1,1]</span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"># 创建训练集对象</span><br><span class="line">trainset = torchvision.datasets.CIFAR10(</span><br><span class="line">    root=&#x27;/path/to/data&#x27;,     # 数据存储路径</span><br><span class="line">    train=True,               # 表示这是训练集</span><br><span class="line">    download=True,            # 如果本地没有数据，则自动下载</span><br><span class="line">    transform=transform       # 应用上面定义的图像预处理操作</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"># 创建训练集的数据加载器 DataLoader</span><br><span class="line">trainloader = torch.utils.data.DataLoader(</span><br><span class="line">    trainset,                  # 使用的数据集</span><br><span class="line">    batch_size=4,              # 每个批次包含4个样本</span><br><span class="line">    shuffle=True,              # 每个 epoch 开始前打乱数据顺序，提高泛化能力</span><br><span class="line">    num_workers=2              # 使用2个子进程加载数据，加快数据读取速度</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"># 创建测试集对象</span><br><span class="line">testset = torchvision.datasets.CIFAR10(</span><br><span class="line">    root=&#x27;/path/to/data&#x27;,      # 数据存储路径</span><br><span class="line">    train=False,               # 表示这是测试集</span><br><span class="line">    download=True,             # 如果本地没有数据，则自动下载</span><br><span class="line">    transform=transform        # 同样应用预处理操作</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"># 创建测试集的数据加载器 DataLoader</span><br><span class="line">testloader = torch.utils.data.DataLoader(</span><br><span class="line">    testset,                   # 使用的数据集</span><br><span class="line">    batch_size=4,              # 每个批次包含4个样本</span><br><span class="line">    shuffle=False,             # 测试时不打乱数据，方便评估模型</span><br><span class="line">    num_workers=2              # 同样使用2个子进程加速数据加载</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"># CIFAR-10 数据集中10个类别的标签名称（用于后续可视化或打印预测结果）</span><br><span class="line">classes = (&#x27;plane&#x27;, &#x27;car&#x27;, &#x27;bird&#x27;, &#x27;cat&#x27;,</span><br><span class="line">           &#x27;deer&#x27;, &#x27;dog&#x27;, &#x27;frog&#x27;, &#x27;horse&#x27;, &#x27;ship&#x27;, &#x27;truck&#x27;)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="3-4-一个完整的训练-推理流程"><a href="#3-4-一个完整的训练-推理流程" class="headerlink" title="3.4 一个完整的训练+推理流程"></a>3.4 一个完整的训练+推理流程</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">import torch.nn as nn</span><br><span class="line">import torch.nn.functional as F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Net(nn.Module):</span><br><span class="line">  def __init__(self):</span><br><span class="line">    super(Net, self).__init__()</span><br><span class="line">    self.conv1 = nn.Conv2d(3, 6, 5)</span><br><span class="line">    self.pool = nn.MaxPool2d(2, 2)</span><br><span class="line">    self.conv2 = nn.Conv2d(6, 16, 5)</span><br><span class="line">    self.fc1 = nn.Linear(16 * 5 * 5, 120)</span><br><span class="line">    self.fc2 = nn.Linear(120, 84)</span><br><span class="line">    self.fc3 = nn.Linear(84, 10)</span><br><span class="line"></span><br><span class="line">  def forward(self, x):</span><br><span class="line">    x = self.pool(F.relu(self.conv1(x)))</span><br><span class="line">    x = self.pool(F.relu(self.conv2(x)))</span><br><span class="line">    x = x.view(-1, 16 * 5 * 5)</span><br><span class="line">    x = F.relu(self.fc1(x))</span><br><span class="line">    x = F.relu(self.fc2(x))</span><br><span class="line">    x = self.fc3(x)</span><br><span class="line">    return x</span><br><span class="line"></span><br><span class="line">net = Net()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>使用交叉熵损失函数，Optimizer使用带冲量的SGD。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import torch.optim as optim</span><br><span class="line"></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)</span><br></pre></td></tr></table></figure>

<p>遍历DataLoader进行训练。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">for epoch in range(2):  # 这里只迭代2个epoch，实际应该进行更多次训练</span><br><span class="line"></span><br><span class="line">  running_loss = 0.0</span><br><span class="line">  for i, data in enumerate(trainloader, 0):</span><br><span class="line">    # 得到输入</span><br><span class="line">    inputs, labels = data</span><br><span class="line"></span><br><span class="line">    # 梯度清零</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    # forward + backward + optimize</span><br><span class="line">    outputs = net(inputs)</span><br><span class="line">    loss = criterion(outputs, labels)</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    # 定义统计信息</span><br><span class="line">    running_loss += loss.item()</span><br><span class="line">    if i % 2000 == 1999:</span><br><span class="line">      print(&#x27;[%d, %5d] loss: %.3f&#x27; %</span><br><span class="line">        (epoch + 1, i + 1, running_loss / 2000))</span><br><span class="line">    running_loss = 0.0</span><br><span class="line"></span><br><span class="line">print(&#x27;Finished Training&#x27;)</span><br></pre></td></tr></table></figure>

<p>在测试集上进行测试。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">correct = 0</span><br><span class="line">total = 0</span><br><span class="line">with torch.no_grad():</span><br><span class="line">for data in testloader:</span><br><span class="line">  images, labels = data</span><br><span class="line">  outputs = net(images)</span><br><span class="line">  _, predicted = torch.max(outputs.data, 1)</span><br><span class="line">  total += labels.size(0)</span><br><span class="line">  correct += (predicted == labels).sum().item()</span><br><span class="line"></span><br><span class="line">print(&#x27;Accuracy of the network on the 10000 test images: %d %%&#x27; % (</span><br><span class="line">  100 * correct / total))</span><br></pre></td></tr></table></figure>

<h3 id="3-5-在GPU上训练"><a href="#3-5-在GPU上训练" class="headerlink" title="3.5 在GPU上训练"></a>3.5 在GPU上训练</h3><p>使用 <code>torch.device</code> 将Tensor移到GPU上</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)</span><br><span class="line">print(device)</span><br></pre></td></tr></table></figure>

<p>使用GPU进行训练</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">class Net2(nn.Module):</span><br><span class="line">def __init__(self):</span><br><span class="line">super(Net2, self).__init__()</span><br><span class="line">self.conv1 = nn.Conv2d(3, 6, 5).to(device)</span><br><span class="line">self.pool = nn.MaxPool2d(2, 2).to(device)</span><br><span class="line">self.conv2 = nn.Conv2d(6, 16, 5).to(device)</span><br><span class="line">self.fc1 = nn.Linear(16 * 5 * 5, 120).to(device)</span><br><span class="line">self.fc2 = nn.Linear(120, 84).to(device)</span><br><span class="line">self.fc3 = nn.Linear(84, 10).to(device)</span><br><span class="line"></span><br><span class="line">def forward(self, x):</span><br><span class="line">x = self.pool(F.relu(self.conv1(x)))</span><br><span class="line">x = self.pool(F.relu(self.conv2(x)))</span><br><span class="line">x = x.view(-1, 16 * 5 * 5)</span><br><span class="line">x = F.relu(self.fc1(x))</span><br><span class="line">x = F.relu(self.fc2(x))</span><br><span class="line">x = self.fc3(x)</span><br><span class="line">return x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = Net2()</span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)</span><br><span class="line"></span><br><span class="line">for epoch in range(20):</span><br><span class="line"></span><br><span class="line">  running_loss = 0.0</span><br><span class="line">  for i, data in enumerate(trainloader, 0):</span><br><span class="line">    # 得到输入</span><br><span class="line">    inputs, labels = data</span><br><span class="line">    inputs, labels = inputs.to(device), labels.to(device)</span><br><span class="line">    # 梯度清零</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    # forward + backward + optimize</span><br><span class="line">    outputs = net(inputs)</span><br><span class="line">    loss = criterion(outputs, labels)</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    # 定义统计信息</span><br><span class="line">    running_loss += loss.item()</span><br><span class="line">    if i % 2000 == 1999:</span><br><span class="line">      print(&#x27;[%d, %5d] loss: %.3f&#x27; %</span><br><span class="line">        (epoch + 1, i + 1, running_loss / 2000))</span><br><span class="line">      running_loss = 0.0</span><br><span class="line"></span><br><span class="line">    print(&#x27;Finished Training&#x27;)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="3-6-使用autograd来实现三层神经网络"><a href="#3-6-使用autograd来实现三层神经网络" class="headerlink" title="3.6 使用autograd来实现三层神经网络"></a>3.6 使用autograd来实现三层神经网络</h3><p>实现一个全连接网络，只有一个隐层而且没有bias，使用欧氏距离作为损失函数。使用PyTorch的Tensor来计算前向阶段，然后使用PyTorch的autograd来自动帮我们反向计算梯度。PyTorch的Tensor代表了计算图中的一个节点。如果x是一个Tensor并且x.requires_grad&#x3D;True，那么x.grad这个Tensor会保存某个scalar(通常是loss)对x的梯度。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line"></span><br><span class="line">dtype = torch.float</span><br><span class="line">device = torch.device(&quot;cpu&quot;)</span><br><span class="line"></span><br><span class="line"># N是batch size；D_in是输入大小</span><br><span class="line"># H是隐层的大小；D_out是输出大小</span><br><span class="line">N, D_in, H, D_out = 64, 1000, 100, 10</span><br><span class="line"></span><br><span class="line"># 创建随机的Tensor作为输入和输出</span><br><span class="line"># 输入和输出需要的requires_grad=False(默认)</span><br><span class="line">x = torch.randn(N, D_in, device=device, dtype=dtype)</span><br><span class="line">y = torch.randn(N, D_out, device=device, dtype=dtype)</span><br><span class="line"></span><br><span class="line"># 创建weight的Tensor，需要设置requires_grad=True</span><br><span class="line">w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)</span><br><span class="line">w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)</span><br><span class="line"></span><br><span class="line">learning_rate = 1e-6</span><br><span class="line">for t in range(500):</span><br><span class="line">  # Forward阶段: mm实现矩阵乘法，但是它不支持broadcasting。</span><br><span class="line">  # 如果需要broadcasting，可以使用matmul</span><br><span class="line">  # clamp本来的用途是把值clamp到指定的范围，这里实现ReLU。</span><br><span class="line">  y_pred = x.mm(w1).clamp(min=0).mm(w2)</span><br><span class="line"></span><br><span class="line">  # pow(2)实现平方计算。</span><br><span class="line">  # loss.item()得到这个tensor的值。也可以直接打印loss，这会打印很多附加信息。</span><br><span class="line">  loss = (y_pred - y).pow(2).sum()</span><br><span class="line">  print(t, loss.item())</span><br><span class="line"></span><br><span class="line">  # 使用autograd进行反向计算。它会计算loss对所有对它有影响的</span><br><span class="line">  # requires_grad=True的Tensor的梯度。</span><br><span class="line"></span><br><span class="line">  loss.backward()</span><br><span class="line"></span><br><span class="line">  # 手动使用梯度下降更新参数。一定要把更新的代码放到torch.no_grad()里</span><br><span class="line">  # 否则下面的更新也会计算梯度。后面我们会使用torch.optim.SGD，</span><br><span class="line">  # 它会帮我们管理这些用于更新梯度的计算。</span><br><span class="line"></span><br><span class="line">  with torch.no_grad():</span><br><span class="line">    w1 -= learning_rate * w1.grad</span><br><span class="line">    w2 -= learning_rate * w2.grad</span><br><span class="line"></span><br><span class="line">    # 手动把梯度清零</span><br><span class="line">    w1.grad.zero_()</span><br><span class="line">    w2.grad.zero_()</span><br></pre></td></tr></table></figure>

<h2 id="4-迁移学习"><a href="#4-迁移学习" class="headerlink" title="4. 迁移学习"></a><strong>4. 迁移学习</strong></h2><p>一般来说我们的训练数据量不会很大，很难达到像ImageNet那样上百万的标注数据集。我们可以使用迁移学习来解决训练数据不足的问题。迁移学习里，我们根据训练数据的多少通常可以采取如下方法：</p>
<ul>
<li><p>训练数据很少</p>
<p>  那么我们通常把一个pretraning的网络的大部分固定住，然后只是把最后一个全连接层换成新的(最后一层通常是不一样的，因为分类的数量不同)，然后只训练这一层</p>
</li>
<li><p>训练数据较多</p>
<p>  我们可以把pretraining的网络的前面一些层固定住，但后面的层不固定，把最后一层换新的，然后训练</p>
</li>
<li><p>训练数据很多</p>
<p>  所有的pretraining的层都可以fine-tuning，只是用pretraining的参数作为初始化参数。</p>
</li>
</ul>
<h3 id="4-1-引入依赖"><a href="#4-1-引入依赖" class="headerlink" title="4.1 引入依赖"></a>4.1 引入依赖</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">from __future__ import print_function, division</span><br><span class="line"></span><br><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line">import torch.optim as optim</span><br><span class="line">from torch.optim import lr_scheduler</span><br><span class="line">import numpy as np</span><br><span class="line">import torchvision</span><br><span class="line">from torchvision import datasets, models, transforms</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import time</span><br><span class="line">import os</span><br><span class="line">import copy</span><br><span class="line"></span><br><span class="line">plt.ion()</span><br></pre></td></tr></table></figure>

<h3 id="4-2-加载数据"><a href="#4-2-加载数据" class="headerlink" title="4.2 加载数据"></a>4.2 加载数据</h3><p>使用torchvision和torch.utils.data包来加载数据。我们要解决的问题是训练一个模型来区分蚂蚁和蜜蜂，每个类别我们大概有120个训练数据，另外每个类有75个验证数据。这是一个很小的训练集，如果直接用一个神经网络来训练，效果会很差。现在我们用迁移学习来解决这个问题。数据可以在<a target="_blank" rel="noopener" href="https://download.pytorch.org/tutorial/hymenoptera_data.zip">这里</a>下载，下载后请解压到data目录下。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"># 训练时会做数据增强和归一化</span><br><span class="line"># 验证的时候只做归一化</span><br><span class="line">data_transforms = &#123;</span><br><span class="line">  &#x27;train&#x27;: transforms.Compose([</span><br><span class="line">    transforms.RandomResizedCrop(224),</span><br><span class="line">    transforms.RandomHorizontalFlip(),</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])</span><br><span class="line">  ]),</span><br><span class="line">  &#x27;val&#x27;: transforms.Compose([</span><br><span class="line">    transforms.Resize(256),</span><br><span class="line">    transforms.CenterCrop(224),</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])</span><br><span class="line">  ]),</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">data_dir = &#x27;../data/hymenoptera_data&#x27;</span><br><span class="line">image_datasets = &#123;x: datasets.ImageFolder(os.path.join(data_dir, x),</span><br><span class="line">    data_transforms[x])</span><br><span class="line">  for x in [&#x27;train&#x27;, &#x27;val&#x27;]&#125;</span><br><span class="line">dataloaders = &#123;x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,</span><br><span class="line">    shuffle=True, num_workers=4)</span><br><span class="line">  for x in [&#x27;train&#x27;, &#x27;val&#x27;]&#125;</span><br><span class="line">dataset_sizes = &#123;x: len(image_datasets[x]) for x in [&#x27;train&#x27;, &#x27;val&#x27;]&#125;</span><br><span class="line">class_names = image_datasets[&#x27;train&#x27;].classes</span><br><span class="line"></span><br><span class="line">device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)</span><br></pre></td></tr></table></figure>

<h3 id="4-3-训练模型"><a href="#4-3-训练模型" class="headerlink" title="4.3 训练模型"></a>4.3 训练模型</h3><p>在此实现一个用于训练模型的通用函数，包括：</p>
<ul>
<li>learning rate的自适应</li>
<li>保存最好的模型</li>
</ul>
<figure class="highlight jsx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line">def <span class="title function_">train_model</span>(model, criterion, optimizer, shceduler, num_epochs=<span class="number">25</span>):</span><br><span class="line">				since = time.<span class="title function_">time</span>()</span><br><span class="line">				</span><br><span class="line">				best_model_wts = copy.<span class="title function_">deepcopy</span>(model.<span class="title function_">state_dict</span>())</span><br><span class="line">				best_acc = <span class="number">0.0</span></span><br><span class="line">				</span><br><span class="line">				<span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="title function_">range</span>(num_epochs):</span><br><span class="line">								<span class="title function_">print</span>(<span class="string">&#x27;Epoch &#123;&#125;/&#123;&#125;&#x27;</span>.<span class="title function_">format</span>(epoch, num_epochs - <span class="number">1</span>))</span><br><span class="line">								<span class="title function_">print</span>(<span class="string">&#x27;-&#x27;</span> * <span class="number">10</span>)</span><br><span class="line">								</span><br><span class="line">								# 每个epoch都分为训练和验证阶段</span><br><span class="line">								<span class="keyword">for</span> phase <span class="keyword">in</span> [<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;val&#x27;</span>]:</span><br><span class="line">												<span class="keyword">if</span> phase == <span class="string">&#x27;train&#x27;</span>:</span><br><span class="line">																scheduler.<span class="title function_">step</span>()</span><br><span class="line">																model.<span class="title function_">train</span>()   # 训练阶段</span><br><span class="line">												<span class="attr">else</span>:</span><br><span class="line">																model.<span class="built_in">eval</span>()    #验证阶段</span><br><span class="line">																</span><br><span class="line">												running_loss = <span class="number">0.0</span></span><br><span class="line">												running_corrects = <span class="number">0</span></span><br><span class="line">												</span><br><span class="line">												# 变量数据集</span><br><span class="line">												<span class="keyword">for</span> inputs, labels <span class="keyword">in</span> dataloaders[phase]:</span><br><span class="line">																inputs = inputs.<span class="title function_">to</span>(device)</span><br><span class="line">																labels = labels.<span class="title function_">to</span>(device)</span><br><span class="line">																</span><br><span class="line">												# 清空参数梯度</span><br><span class="line">												optimizer.<span class="title function_">zero_grad</span>()</span><br><span class="line">												</span><br><span class="line">												# forward</span><br><span class="line">												# 只有训练的时候track用于梯度计算的历史信息</span><br><span class="line">												<span class="keyword">with</span> torch.<span class="title function_">set_grad_enable</span>(phase == <span class="string">&#x27;train&#x27;</span>):</span><br><span class="line">																outputs = <span class="title function_">model</span>(inputs)</span><br><span class="line">																_, preds = torch.<span class="title function_">max</span>(outputs, <span class="number">1</span>)</span><br><span class="line">																loss = <span class="title function_">criterion</span>(outputs, labels)</span><br><span class="line">																</span><br><span class="line">																# 如果是训练，需要backward和更新参数</span><br><span class="line">																<span class="keyword">if</span> phase == <span class="string">&#x27;train&#x27;</span>:</span><br><span class="line">																				loss.<span class="title function_">backward</span>()</span><br><span class="line">																				optimizer.<span class="title function_">step</span>()</span><br><span class="line">																				</span><br><span class="line">												runnning_loss += loss.<span class="title function_">item</span>() * inputs.<span class="title function_">size</span>(<span class="number">0</span>)</span><br><span class="line">												running_corrects += torch.<span class="title function_">sum</span>(preds == labels.<span class="property">data</span>)</span><br><span class="line">												</span><br><span class="line">												epoch_loss = running_loss / dataset_sizes[phase]</span><br><span class="line">												epoch_corrects = running_corrects.<span class="title function_">double</span>() / dataset_sizes[phase]</span><br><span class="line">												</span><br><span class="line">												<span class="title function_">print</span>(<span class="string">&#x27;&#123;&#125; Loss: &#123;:.4f&#125; Acc: &#123;:.4f&#125;&#x27;</span>.<span class="title function_">format</span>(</span><br><span class="line">																phase, epoch_loss, epoch_acc))</span><br><span class="line">																</span><br><span class="line">												# 保存验证集上的最佳模型</span><br><span class="line">												<span class="keyword">if</span> phase == <span class="string">&#x27;val&#x27;</span> and epoch_acc &gt; <span class="attr">best_acc</span>:</span><br><span class="line">																best_acc = epoch_acc</span><br><span class="line">																best_model_wts = copy.<span class="title function_">deepcopy</span>(model.<span class="title function_">state_dict</span>())</span><br><span class="line">												<span class="title function_">print</span>()</span><br><span class="line">								</span><br><span class="line">				time_elapsed = time.<span class="title function_">time</span>() - since</span><br><span class="line">				<span class="title function_">print</span>(<span class="string">&#x27;Training complete in &#123;:.0f&#125;m &#123;:.0f&#125;s&#x27;</span>.<span class="title function_">format</span>(</span><br><span class="line">time_elapsed <span class="comment">// 60, time_elapsed % 60))</span></span><br><span class="line">				<span class="title function_">print</span>(<span class="string">&#x27;Best val Acc: &#123;:4f&#125;&#x27;</span>.<span class="title function_">format</span>(best_acc))</span><br><span class="line">				</span><br><span class="line">				# 加载最优模型</span><br><span class="line">				model.<span class="title function_">load_state_dict</span>(best_model_wts)</span><br><span class="line">				<span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>

<h3 id="4-4-可视化预测结果的函数"><a href="#4-4-可视化预测结果的函数" class="headerlink" title="4.4 可视化预测结果的函数"></a>4.4 可视化预测结果的函数</h3><figure class="highlight jsx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">def <span class="title function_">visualize_model</span>(model, num_images=<span class="number">6</span>):</span><br><span class="line">				was_training = model.<span class="property">training</span></span><br><span class="line">				model.<span class="built_in">eval</span>()</span><br><span class="line">				images_so_far = <span class="number">0</span></span><br><span class="line">				fig = plt.<span class="title function_">figure</span>()</span><br><span class="line">				</span><br><span class="line">				<span class="keyword">with</span> torch.<span class="title function_">no_grad</span>():</span><br><span class="line">								<span class="keyword">for</span> i, (inputs, labels) <span class="keyword">in</span> <span class="title function_">enumerate</span>(dataloaders[<span class="string">&#x27;val&#x27;</span>]):</span><br><span class="line">												inputs = inputs.<span class="title function_">to</span>(device)</span><br><span class="line">												labels = labels.<span class="title function_">to</span>(device)</span><br><span class="line">												</span><br><span class="line">												outputs = <span class="title function_">model</span>(inputs)</span><br><span class="line">												_, preds = torch.<span class="title function_">max</span>(outputs, <span class="number">1</span>)</span><br><span class="line">												</span><br><span class="line">												<span class="keyword">for</span> j <span class="keyword">in</span> <span class="title function_">range</span>(inputs.<span class="title function_">size</span>()[<span class="number">0</span>]):</span><br><span class="line">																images_so_far += <span class="number">1</span></span><br><span class="line">																ax = plt.<span class="title function_">subplot</span>(num_images<span class="comment">//2, 2, images_so_far)</span></span><br><span class="line">																ax.<span class="title function_">axis</span>(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">																ax.<span class="title function_">set_title</span>(<span class="string">&#x27;predicted: &#123;&#125;&#x27;</span>.<span class="title function_">format</span>(class_names[preds[j]]))</span><br><span class="line">																<span class="title function_">imshow</span>(inputs.<span class="title function_">cpu</span>().<span class="property">data</span>[j])</span><br><span class="line">																</span><br><span class="line">																<span class="keyword">if</span> images_so_far == <span class="attr">num_images</span>:</span><br><span class="line">																				model.<span class="title function_">train</span>(mode=was_training)</span><br><span class="line">																				<span class="keyword">return</span></span><br><span class="line">								model.<span class="title function_">train</span>(mode=was_training)</span><br></pre></td></tr></table></figure>

<h3 id="4-5-fine-tuning-所有参数"><a href="#4-5-fine-tuning-所有参数" class="headerlink" title="4.5 fine-tuning 所有参数"></a>4.5 fine-tuning 所有参数</h3><p>首先加载一个预训练的模型(imagenet上的resnet)，因为我们的类别数和imagenet不同，所以我们需要删掉原来的全连接层，换成新的全连接层。这里我们让所有的模型参数都可以调整，包括新加的全连接层和预训练的层。</p>
<figure class="highlight jsx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">model_ft = models.<span class="title function_">resent18</span>(pretrained=<span class="title class_">True</span>) # 加载预训练模型</span><br><span class="line">num_ftrs = model_ft.<span class="property">fc</span>.<span class="property">in_features</span> # 获取最后一层输入特征值</span><br><span class="line">model_ft.<span class="property">fc</span> = nn.<span class="title class_">Linear</span>(num_ftrs, <span class="number">2</span>) # 替换最后一层全连接层</span><br><span class="line"></span><br><span class="line">model_ft = model_ft.<span class="title function_">to</span>(device)</span><br><span class="line"></span><br><span class="line"># 所有参数都可以训练</span><br><span class="line">optimizer_ft = optim.<span class="title function_">SGD</span>(model_ft.<span class="title function_">parameters</span>(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line"># 每<span class="number">7</span>个epoch learning rate变为原来的<span class="number">10</span>% </span><br><span class="line">exp_lr_scheduler = lr_scheduler.<span class="title class_">StepLR</span>(optimizer_ft, step_size=<span class="number">7</span>, gamma=<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line">model_ft = <span class="title function_">train_model</span>(model_ft, criterion, optimizer_ft, exp_lr_scheduler,</span><br><span class="line">	num_epochs=<span class="number">25</span>)</span><br></pre></td></tr></table></figure>

<h3 id="4-6-fine-tuning-最后一层参数"><a href="#4-6-fine-tuning-最后一层参数" class="headerlink" title="4.6 fine-tuning 最后一层参数"></a>4.6 fine-tuning 最后一层参数</h3><p>我们用可以固定住前面层的参数，只训练最后一层。这比之前要快将近一倍，因为反向计算梯度只需要计算最后一层。但是前向计算的时间是一样的。</p>
<figure class="highlight jsx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">model_conv = torchvision.<span class="property">models</span>.<span class="title function_">resnet18</span>(pretrained=<span class="title class_">True</span>)</span><br><span class="line"></span><br><span class="line"># 冻结所有参数</span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> model_conv.<span class="title function_">parameters</span>():</span><br><span class="line">	param.<span class="property">requires_grad</span> = <span class="title class_">False</span></span><br><span class="line"></span><br><span class="line"># 新加的层默认requires_grad=<span class="title class_">True</span> </span><br><span class="line">num_ftrs = model_conv.<span class="property">fc</span>.<span class="property">in_features</span></span><br><span class="line">model_conv.<span class="property">fc</span> = nn.<span class="title class_">Linear</span>(num_ftrs, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">model_conv = model_conv.<span class="title function_">to</span>(device)</span><br><span class="line"></span><br><span class="line">criterion = nn.<span class="title class_">CrossEntropyLoss</span>()</span><br><span class="line"></span><br><span class="line"># 值训练最后一个全连接层。</span><br><span class="line">optimizer_conv = optim.<span class="title function_">SGD</span>(model_conv.<span class="property">fc</span>.<span class="title function_">parameters</span>(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line">exp_lr_scheduler = lr_scheduler.<span class="title class_">StepLR</span>(optimizer_conv, step_size=<span class="number">7</span>, gamma=<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line">model_conv = <span class="title function_">train_model</span>(model_conv, criterion, optimizer_conv,</span><br><span class="line">	exp_lr_scheduler, num_epochs=<span class="number">25</span>)</span><br></pre></td></tr></table></figure>

    </div>

    
    <div class="about">
        <h1>About this Post</h1>
        <div class="details">
            <p>This post is written by Latteo, licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc/4.0">CC BY-NC 4.0</a>.</p>
        </div>
        
    </div>
    

    <div class="container post-prev-next">
        <a class="next"></a>
        
        <a href="/2025/06/24/start-my-blog/" class="prev">
            <div>
                <div class="text">
                    <p class="label">Previous</p>
                    <h3 class="title">start_my_blog</>
                </div>
            </div>
        </a>
        
    </div>

    
        
        
    
</article>

        <footer>
    <div class="inner">
        <div class="links">
            
        </div>
        <span>&copy; 2025 Latteo<br>Powered by <a href="http://hexo.io/" target="_blank">Hexo</a> </span>
        
        
            <br>
            <div class="color-scheme-toggle" role="radiogroup" id="theme-color-scheme-toggle">
                <label>
                    <input type="radio" value="light">
                    <span>Light</span>
                </label>
                <label>
                    <input type="radio" value="dark">
                    <span>Dark</span>
                </label>
                <label>
                    <input type="radio" value="auto">
                    <span>Auto</span>
                </label>
            </div>
        
    </div>
</footer>


        
<script src="/js/main.js"></script>

        
        
        

        
        <script src="https://unpkg.com/scrollreveal"></script>
        <script>
            window.addEventListener('load', () => {
                ScrollReveal({ delay: 250, reset: true, easing: 'cubic-bezier(0, 0, 0, 1)' })
                ScrollReveal().reveal('.post-list-item .cover-img img')
                ScrollReveal().reveal('.post-list-item, .card, .content p img, .content .block-large img', { distance: '60px', origin: 'bottom', duration: 800 })
            })
        </script>
        
    </body>
</html>